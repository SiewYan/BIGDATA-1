{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Studying tunning of spark\n",
    "\n",
    "1.) __spark.cores.max__ is the maximum number of core.\n",
    "\n",
    "2.) __spark.executor.cores__ property controls the number of concurrent tasks an executor can run. (__spark.executor.cores=5__ means that each executor can run a maximum of five tasks at the same time)\n",
    "\n",
    "3.) __spark.executor.memory__ The memory property impacts the amount of data Spark can cache, as well as the maximum sizes of the shuffle data structures used for grouping, aggregations, and joins.\n",
    "\n",
    "4.) __spark.executor.instances__ configuration property control the number of executors requested. (Spark 1.3, you will be able to avoid setting this property by turning on dynamic allocation with the __spark.dynamicAllocation.enabled__ property. Dynamic allocation enables a Spark application to request executors when there is a backlog of pending tasks and free up executors when idle.)\n",
    "\n",
    "\n",
    "\n",
    "Notice:\n",
    "\n",
    "1.) high number of core per executor will lead to bad HDFS I/O throughput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql\n",
    "session = pyspark.sql.SparkSession.builder \\\n",
    "    .master('spark://10.64.22.215:7077') \\\n",
    "    .appName('Zpeak_Nanoaod-SPARK-Histbook') \\\n",
    "    .config('spark.jars.packages','org.diana-hep:spark-root_2.11:0.1.16,org.diana-hep:histogrammar-sparksql_2.11:1.0.4') \\\n",
    "    .config('spark.driver.extraClassPath','/opt/hadoop/share/hadoop/common/lib/EOSfs.jar') \\\n",
    "    .config('spark.executor.extraClassPath','/opt/hadoop/share/hadoop/common/lib/EOSfs.jar') \\\n",
    "    .config('spark.sql.caseSensitive','true') \\\n",
    "    .config('spark.serializer','org.apache.spark.serializer.KryoSerializer') \\\n",
    "    .config('spark.cores.max','40') \\\n",
    "    .config('spark.executor.instances','11') \\\n",
    "    .config('spark.executor.cores','3') \\\n",
    "    .config('spark.executor.memory','1g') \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "sqlContext = session\n",
    "print 'Spark version: ',sqlContext.version\n",
    "print 'SparkSQL sesssion created'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "from samples import *\n",
    "\n",
    "DFList = [] \n",
    "\n",
    "for s in samples:\n",
    "    print 'Loading {0} sample from EOS file'.format(s) \n",
    "    dsPath = \"root://eospublic.cern.ch//eos/opstest/cmspd-bigdata/\"+samples[s]['filename']    \n",
    "    tempDF = sqlContext.read \\\n",
    "                .format(\"org.dianahep.sparkroot\") \\\n",
    "                .option(\"tree\", \"Events\") \\\n",
    "                .load(dsPath)\\\n",
    "                .withColumn(\"pseudoweight\", lit(samples[s]['weight'])) \\\n",
    "                .withColumn(\"sample\", lit(s))                \n",
    "    DFList.append(tempDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define interesting attributes to be selected\n",
    "columns = [\n",
    "    ### MUON\n",
    "    'Muon_pt',\n",
    "    'Muon_eta',\n",
    "    'Muon_phi',\n",
    "    'Muon_mass',\n",
    "    'Muon_charge',\n",
    "    'Muon_mediumId',\n",
    "    'Muon_softId',\n",
    "    'Muon_tightId',\n",
    "    'nMuon',\n",
    "    ### SAMPLE\n",
    "    'sample',\n",
    "    ### Jet\n",
    "    'nJet',\n",
    "    'Jet_pt',\n",
    "    'Jet_eta',\n",
    "    'Jet_phi',\n",
    "    'Jet_mass',\n",
    "    'Jet_bReg',\n",
    "    ### Weight\n",
    "    'pseudoweight',\n",
    "]\n",
    "\n",
    "# Select columns from dataframe\n",
    "DF = DFList[0].select(columns)\n",
    "\n",
    "# Merge all dataset into a single dataframe\n",
    "for df_ in DFList[1:]:\n",
    "    DF = DF.union(df_.select(columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'Partition allocated for Dataframe:',DF.rdd.getNumPartitions(), 'partition'\n",
    "print 'Partition allocated for Dataframe reported from executors (JVM):',sqlContext._jsc.sc().getExecutorMemoryStatus().size(), 'partition'\n",
    "print 'Default number of partition (defaultParallelism) = ',sqlContext._jsc.sc().defaultParallelism()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext._jsc._conf.getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
