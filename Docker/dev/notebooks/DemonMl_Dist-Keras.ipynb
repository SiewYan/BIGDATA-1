{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up Spark\n",
    "\n",
    "Start a pySpark session including Diana-Hep spark-root and histogrammar APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.1\n",
      "SparkSQL sesssion created\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql\n",
    "\n",
    "#### CLUSTER CONFIG ####\n",
    "# 5 Nodes\n",
    "# 2 cores per Node\n",
    "# 2.9 GB RAM per Node\n",
    "#######################\n",
    "\n",
    "num_cores = 2 # core per executer\n",
    "num_executors = 5\n",
    "\n",
    "session = pyspark.sql.SparkSession.builder \\\n",
    "    .master('spark://10.64.22.198:7077') \\\n",
    "    .appName('DemoWithML') \\\n",
    "    .config('spark.jars.packages','org.diana-hep:spark-root_2.11:0.1.16,org.diana-hep:histogrammar-sparksql_2.11:1.0.4') \\\n",
    "    .config('spark.driver.extraClassPath','/opt/hadoop/share/hadoop/common/lib/EOSfs.jar') \\\n",
    "    .config('spark.executor.extraClassPath','/opt/hadoop/share/hadoop/common/lib/EOSfs.jar') \\\n",
    "    .config(\"spark.serializer\",\"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config('spark.cores.max','10') \\\n",
    "    .config('spark.executor.memory','2g') \\\n",
    "    .config('spark.executor.cores',`num_cores`) \\\n",
    "    .config('spark.executor.instances', `num_executors`) \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "sqlContext = session\n",
    "print sqlContext.version\n",
    "\n",
    "print 'SparkSQL sesssion created'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading root files stored remotely on EOS via xrootd\n",
    "\n",
    "Loading root files (NanoAOD CMS format) from CERN public EOS area via xrootd.\n",
    "Trees are read using the spark-root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading TT sample from EOS file\n",
      "Loading WW sample from EOS file\n",
      "Loading SingleMuon sample from EOS file\n",
      "Loading ZZ sample from EOS file\n",
      "Loading DYJetsToLL sample from EOS file\n",
      "Loading WZ sample from EOS file\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "from samples import *\n",
    "\n",
    "DFList = [] \n",
    "\n",
    "for s in samples:\n",
    "    print 'Loading {0} sample from EOS file'.format(s) \n",
    "    dsPath = \"root://eospublic.cern.ch//eos/opstest/cmspd-bigdata/\"+samples[s]['filename']    \n",
    "    tempDF = sqlContext.read \\\n",
    "                .format(\"org.dianahep.sparkroot\") \\\n",
    "                .option(\"tree\", \"Events\") \\\n",
    "                .load(dsPath) \\\n",
    "                .withColumn(\"pseudoweight\", lit(samples[s]['weight']))\\\n",
    "                .withColumn(\"sample\", lit(s))                \n",
    "    DFList.append(tempDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Access DataFrame content\n",
    "\n",
    "Get list of columns in the DataFrame (\"branches\" of the equivalent ROOT TTree)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tempDF = sqlContext.read.format(\"org.dianahep.sparkroot.experimental\").option(\"tree\", \"Events\") \\\n",
    "#        .load(\"root://eospublic.cern.ch//eos/opstest/cmspd-bigdata/SingleMuonRun2016C-03Feb2017-v1.root\")\n",
    "\n",
    "#tempDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data reduction\n",
    "\n",
    "Subsets of interesting attributes can be selected via 'select' operations on the DataFrames (equivalent to \"pruning\" steps in ROOT-based frameworks).\n",
    "\n",
    "All datasets can be joint into a single DataFrame (e.g. collecting data from various samples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Muon_pt: array (nullable = true)\n",
      " |    |-- element: float (containsNull = true)\n",
      " |-- Muon_eta: array (nullable = true)\n",
      " |    |-- element: float (containsNull = true)\n",
      " |-- Muon_phi: array (nullable = true)\n",
      " |    |-- element: float (containsNull = true)\n",
      " |-- Muon_mass: array (nullable = true)\n",
      " |    |-- element: float (containsNull = true)\n",
      " |-- Muon_charge: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      " |-- Muon_mediumId: array (nullable = true)\n",
      " |    |-- element: boolean (containsNull = true)\n",
      " |-- Muon_softId: array (nullable = true)\n",
      " |    |-- element: boolean (containsNull = true)\n",
      " |-- Muon_tightId: array (nullable = true)\n",
      " |    |-- element: boolean (containsNull = true)\n",
      " |-- nMuon: integer (nullable = true)\n",
      " |-- sample: string (nullable = false)\n",
      " |-- Jet_pt: array (nullable = true)\n",
      " |    |-- element: float (containsNull = true)\n",
      " |-- Jet_eta: array (nullable = true)\n",
      " |    |-- element: float (containsNull = true)\n",
      " |-- Jet_phi: array (nullable = true)\n",
      " |    |-- element: float (containsNull = true)\n",
      " |-- Jet_mass: array (nullable = true)\n",
      " |    |-- element: float (containsNull = true)\n",
      " |-- Jet_bReg: array (nullable = true)\n",
      " |    |-- element: float (containsNull = true)\n",
      " |-- pseudoweight: double (nullable = false)\n",
      "\n",
      "Partitions: 6\n"
     ]
    }
   ],
   "source": [
    "columns = [\n",
    "    ### MUON\n",
    "    'Muon_pt',\n",
    "    'Muon_eta',\n",
    "    'Muon_phi',\n",
    "    'Muon_mass',\n",
    "    'Muon_charge',\n",
    "    'Muon_mediumId',\n",
    "    'Muon_softId',\n",
    "    'Muon_tightId',\n",
    "    'nMuon',\n",
    "    ### SAMPLE\n",
    "    'sample',\n",
    "    ### Jet\n",
    "    'Jet_pt',\n",
    "    'Jet_eta',\n",
    "    'Jet_phi',\n",
    "    'Jet_mass',\n",
    "    'Jet_bReg',\n",
    "    ### Weight\n",
    "    'pseudoweight',\n",
    "]\n",
    "\n",
    "# Select columns from dataframe\n",
    "DF = DFList[0].select(columns)\n",
    "DF.printSchema()\n",
    "\n",
    "# Merge all dataset into a single dataframe\n",
    "for df_ in DFList[1:]:\n",
    "    DF = DF.union(df_.select(columns))\n",
    "\n",
    "print 'Partitions: {}'.format(DF.rdd.getNumPartitions()) ## this will show up 7 parallelize in JOB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data selection\n",
    "\n",
    "Selection of events based on features is obtained via a 'filter' operation.\n",
    "\n",
    "Number of entries is obtained by 'count'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print 'total number of events in the DataFrame  = ', DF.count()\n",
    "#print 'events in the DataFrame with \\\"nMuon > 0\\\" = ', DF.filter('nMuon > 0').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caching - 1\n",
    "\n",
    "Dataframes can be cached into memory, shared across the Spark cluster nodes, for a faster access.\n",
    "\n",
    "Caching takes some time..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Muon_pt: array<float>, Muon_eta: array<float>, Muon_phi: array<float>, Muon_mass: array<float>, Muon_charge: array<int>, Muon_mediumId: array<boolean>, Muon_softId: array<boolean>, Muon_tightId: array<boolean>, nMuon: int, sample: string, Jet_pt: array<float>, Jet_eta: array<float>, Jet_phi: array<float>, Jet_mass: array<float>, Jet_bReg: array<float>, pseudoweight: double]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caching - 2\n",
    "\n",
    "... but ensures fast data-handling operations afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of events in the DataFrame  =  2662199\n"
     ]
    }
   ],
   "source": [
    "print 'total number of events in the DataFrame  = ', DF.count()\n",
    "#print 'events in the DataFrame with \\\"nMuon > 0\\\" = ', DF.filter('nMuon > 0').count()\n",
    "#DF['Jet_pt'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data inspection\n",
    "\n",
    "\n",
    "Events can be inspected with 'show' (as in TTree.Show() ), also concatenating 'select' and 'filter'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DF.filter(DF['sample'] == 'DYJetsToLL')\\\n",
    "#  .select('sample','nMuon','Muon_pt','Muon_eta','Muon_phi','Muon_charge','Jet_pt','Jet_eta','Jet_phi')\\\n",
    "#  .show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create derivate quantities and structures - 1\n",
    "\n",
    "User defined functions can be used for transformations evalueted row by row to compute derived quantity, such as invaraint mass of two physics objects involving multiple column.\n",
    "The return value is added as a new column in the output DataFrame.\n",
    "\n",
    "Dimuon candidate structure is created as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "dimuonSchema = StructType([\n",
    "    StructField(\"pass\", BooleanType(), False),   # True if filled / False if default(empty) \n",
    "    #\n",
    "    StructField(\"mass\", FloatType(), False),     # Dimuon mass\n",
    "    StructField(\"pt\", FloatType(), False),       # Dimuon pt\n",
    "    StructField(\"eta\", FloatType(), False),      # Dimuon eta\n",
    "    StructField(\"phi\", FloatType(), False),      # Dimuon phi\n",
    "    StructField(\"dPhi\", FloatType(), False),     # DeltaPhi(mu1,mu2)\n",
    "    StructField(\"dR\", FloatType(), False),       # DeltaR(mu1,mu2)\n",
    "    StructField(\"dEta\", FloatType(), False),     # DeltaEta(mu1,mu2)\n",
    "    #\n",
    "    StructField(\"pt1\", FloatType(), False),   # leading mu pT \n",
    "    StructField(\"pt2\", FloatType(), False),   # sub-leading mu pT \n",
    "    StructField(\"eta1\", FloatType(), False),  # leading mu eta\n",
    "    StructField(\"eta2\", FloatType(), False),  # sub-leading mu eta\n",
    "    StructField(\"phi1\", FloatType(), False),  # leading mu phi\n",
    "    StructField(\"phi2\", FloatType(), False),  # sub-leading mu phi\n",
    "])\n",
    "\n",
    "dijetSchema = StructType([\n",
    "    StructField(\"mass\", FloatType(), False),     # Dijet mass\n",
    "    StructField(\"pt\", FloatType(), False),       # Dijet pt\n",
    "    StructField(\"eta\", FloatType(), False),      # Dijet eta\n",
    "    StructField(\"phi\", FloatType(), False),      # Dijet phi\n",
    "    StructField(\"dPhi\", FloatType(), False),     # DeltaPhi(jet1,jet2)\n",
    "    StructField(\"dR\", FloatType(), False),       # DeltaR(jet1,jet2)\n",
    "    StructField(\"dEta\", FloatType(), False),     # DeltaEta(jet1,jet2)\n",
    "])\n",
    "\n",
    "#def binaryWeight(pt):\n",
    "    #for idx in range(len(pt)):\n",
    "    #    if pt[idx]>50:\n",
    "    #        return 0.\n",
    "    #    else:\n",
    "    #        return 1.\n",
    "\n",
    "def deltaPhi(phi1,phi2):\n",
    "    ## Catch if being called with two objects\n",
    "    if type(phi1) != float and type(phi1) != int:\n",
    "        phi1 = phi1.phi\n",
    "    if type(phi2) != float and type(phi2) != int:\n",
    "        phi2 = phi2.phi\n",
    "    ## Otherwise\n",
    "    dphi = (phi1-phi2)\n",
    "    while dphi >  pi: dphi -= 2*pi\n",
    "    while dphi < -pi: dphi += 2*pi\n",
    "    return dphi\n",
    "\n",
    "def deltaR(eta1,phi1,eta2=None,phi2=None):\n",
    "    ## catch if called with objects\n",
    "    if eta2 == None:\n",
    "        return deltaR(eta1.eta,eta1.phi,phi1.eta,phi1.phi)\n",
    "    ## otherwise\n",
    "    return hypot(eta1-eta2, deltaPhi(phi1,phi2))\n",
    "\n",
    "def invMass(pt1, pt2, eta1, eta2, phi1, phi2, mass1, mass2):\n",
    "    #\n",
    "    theta1 = 2.0*atan(exp(-eta1))\n",
    "    px1    = pt1 * cos(phi1)\n",
    "    py1    = pt1 * sin(phi1)\n",
    "    pz1    = pt1 / tan(theta1)\n",
    "    E1     = sqrt(px1**2 + py1**2 + pz1**2 + mass1**2)\n",
    "\n",
    "    theta2 = 2.0*atan(exp(-eta2))\n",
    "    px2    = pt2 * cos(phi2)\n",
    "    py2    = pt2 * sin(phi2)\n",
    "    pz2    = pt2 / tan(theta2)\n",
    "    E2     = sqrt(px2**2 + py2**2 + pz2**2 + mass2**2)\n",
    "\n",
    "    themass  = sqrt((E1 + E2)**2 - (px1 + px2)**2 - (py1 + py2)**2 - (pz1 + pz2)**2)\n",
    "    thept    = sqrt((px1 + px2)**2 + (py1 + py2)**2)\n",
    "    thetheta = atan( thept / (pz1 + pz2) )        \n",
    "    theeta   = 0.5*log( (sqrt((px1 + px2)**2 + (py1 + py2)**2 + (pz1 + pz2)**2)+(pz1 + pz2))/(sqrt((px1 + px2)**2 + (py1 + py2)**2 + (pz1 + pz2)**2)-(pz1 + pz2)) )\n",
    "    thephi   = asin((py1 + py2)/thept)\n",
    "\n",
    "    delPhi = deltaPhi(phi1,phi2)\n",
    "    delR   = deltaR(eta1,phi1,eta2,phi2)\n",
    "    delEta = eta1-eta2\n",
    "\n",
    "    return (themass, thept, theeta, thephi, delPhi, delR, delEta)\n",
    "\n",
    "def dimuonCandidate(pt, eta, phi, mass, charge, mediumid):\n",
    "    # default class implementation   \n",
    "    default_ = (False, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n",
    "    \n",
    "    \"\"\"\n",
    "    Z->mm candidate from arbitrary muon selection:\n",
    "      N(mu) >= 2\n",
    "      pT > 30, 10\n",
    "      abs(eta) < 2.4, 2.4\n",
    "      mediumId muon\n",
    "      opposite charge\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(pt) < 2:\n",
    "        return default_\n",
    "    \n",
    "    #Identify muon candidate\n",
    "    leadingIdx = None\n",
    "    trailingIdx = None\n",
    " \n",
    "    for idx in range(len(pt)):\n",
    "        if leadingIdx == None:\n",
    "            if pt[idx] > 30 and abs(eta[idx]) < 2.4 and mediumid[idx]:\n",
    "                leadingIdx = idx\n",
    "        elif trailingIdx == None:\n",
    "            if pt[idx] > 10 and abs(eta[idx]) < 2.4 and mediumid[idx]:\n",
    "                trailingIdx = idx\n",
    "        else:\n",
    "            if pt[idx] > 10 and abs(eta[idx]) < 2.4 and mediumid[idx]:\n",
    "                return default_\n",
    "\n",
    "    if leadingIdx != None and trailingIdx != None and charge[leadingIdx] != charge[trailingIdx]:            \n",
    "        # Candidate found\n",
    "        dimuon_   = (True,) + \\\n",
    "                    invMass(pt[leadingIdx], pt[trailingIdx],\n",
    "                            eta[leadingIdx], eta[trailingIdx],\n",
    "                            phi[leadingIdx], phi[trailingIdx],\n",
    "                            mass[leadingIdx], mass[trailingIdx]) + \\\n",
    "                    (pt[leadingIdx], pt[trailingIdx],\n",
    "                     eta[leadingIdx], eta[trailingIdx],\n",
    "                     phi[leadingIdx], phi[trailingIdx])\n",
    "                \n",
    "        return dimuon_\n",
    "    else:\n",
    "        return default_    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create derivate quantities and structures - 2\n",
    "\n",
    "And a generic function filling the candidate structure can be defined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create derivate quantities and structures - 3\n",
    "\n",
    "Finally, a dimuon candidate structure can be appended to the DataFrame as an additional column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o407.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 10, 10.64.22.201, executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark-2.2.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/opt/spark-2.2.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/spark-2.2.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 104, in <lambda>\n    func = lambda _, it: map(mapper, it)\n  File \"<string>\", line 1, in <lambda>\n  File \"/opt/spark-2.2.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 69, in <lambda>\n    return lambda *a: toInternal(f(*a))\n  File \"<ipython-input-9-f10a227e8e97>\", line 67, in invMass\nTypeError: bad operand type for unary -: 'NoneType'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1.apply(BatchEvalPythonExec.scala:144)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1.apply(BatchEvalPythonExec.scala:87)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)\n\tat org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1055)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1029)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:969)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1029)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:760)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:285)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2029)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2050)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2069)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:336)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:2861)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2150)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2150)\n\tat org.apache.spark.sql.Dataset$$anonfun$55.apply(Dataset.scala:2842)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:2841)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2150)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2363)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:241)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark-2.2.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/opt/spark-2.2.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/spark-2.2.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 104, in <lambda>\n    func = lambda _, it: map(mapper, it)\n  File \"<string>\", line 1, in <lambda>\n  File \"/opt/spark-2.2.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 69, in <lambda>\n    return lambda *a: toInternal(f(*a))\n  File \"<ipython-input-9-f10a227e8e97>\", line 67, in invMass\nTypeError: bad operand type for unary -: 'NoneType'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1.apply(BatchEvalPythonExec.scala:144)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1.apply(BatchEvalPythonExec.scala:87)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)\n\tat org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1055)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1029)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:969)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1029)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:760)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:285)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-287c8e57d7f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'DeltaRZjj'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDeltaRZJJUDF\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m'Dimuon.eta'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Dimuon.phi'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Dijet.eta'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Dijet.phi'\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Dimuon.pass == True'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Dimuon'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;31m#DF.where('Dimuon.pass == True').select('pseudoweight').show(3)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.2.1-bin-hadoop2.7/python/pyspark/sql/dataframe.pyc\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate)\u001b[0m\n\u001b[1;32m    334\u001b[0m         \"\"\"\n\u001b[1;32m    335\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.2.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.2.1-bin-hadoop2.7/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.2.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o407.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 10, 10.64.22.201, executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark-2.2.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/opt/spark-2.2.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/spark-2.2.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 104, in <lambda>\n    func = lambda _, it: map(mapper, it)\n  File \"<string>\", line 1, in <lambda>\n  File \"/opt/spark-2.2.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 69, in <lambda>\n    return lambda *a: toInternal(f(*a))\n  File \"<ipython-input-9-f10a227e8e97>\", line 67, in invMass\nTypeError: bad operand type for unary -: 'NoneType'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1.apply(BatchEvalPythonExec.scala:144)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1.apply(BatchEvalPythonExec.scala:87)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)\n\tat org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1055)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1029)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:969)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1029)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:760)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:285)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2029)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2050)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2069)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:336)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:2861)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2150)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2150)\n\tat org.apache.spark.sql.Dataset$$anonfun$55.apply(Dataset.scala:2842)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:2841)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2150)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2363)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:241)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark-2.2.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/opt/spark-2.2.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/spark-2.2.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 104, in <lambda>\n    func = lambda _, it: map(mapper, it)\n  File \"<string>\", line 1, in <lambda>\n  File \"/opt/spark-2.2.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 69, in <lambda>\n    return lambda *a: toInternal(f(*a))\n  File \"<ipython-input-9-f10a227e8e97>\", line 67, in invMass\nTypeError: bad operand type for unary -: 'NoneType'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1.apply(BatchEvalPythonExec.scala:144)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1.apply(BatchEvalPythonExec.scala:87)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)\n\tat org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1055)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1029)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:969)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1029)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:760)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:285)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "dimuonUDF = udf(dimuonCandidate, dimuonSchema)\n",
    "DeltaRZJJUDF = udf(deltaR, FloatType())\n",
    "dijetUDF = udf(invMass, dijetSchema)\n",
    "\n",
    "\n",
    "#biweightUDF = udf(binaryWeight, FloatType())\n",
    "\n",
    "DF = DF.withColumn('Dimuon', dimuonUDF (\"Muon_pt\",\n",
    "                                        \"Muon_eta\",\n",
    "                                        \"Muon_phi\",\n",
    "                                        \"Muon_mass\",\n",
    "                                        \"Muon_charge\",\n",
    "                                        \"Muon_mediumId\")\n",
    "                  )\n",
    "\n",
    "DF = DF.withColumn('Dijet', dijetUDF ( DF[\"Jet_pt\"][0],\n",
    "                                       DF[\"Jet_pt\"][1],\n",
    "                                       DF[\"Jet_eta\"][0],\n",
    "                                       DF[\"Jet_eta\"][1],\n",
    "                                       DF[\"Jet_phi\"][0],\n",
    "                                       DF[\"Jet_phi\"][1],\n",
    "                                       DF[\"Jet_mass\"][0],\n",
    "                                       DF[\"Jet_mass\"][1]\n",
    "                                     )\n",
    "                  )\n",
    "\n",
    "#DF = DF.withColumn('pseudoweight', biweightUDF(\"Muon_pt\"))\n",
    "\n",
    "DF = DF.withColumn('DeltaRZjj', DeltaRZJJUDF( 'Dimuon.eta', 'Dimuon.phi', 'Dijet.eta', 'Dijet.phi' ) )\n",
    "\n",
    "DF.where('Dimuon.pass == True').select('Dimuon').show(3)\n",
    "#DF.where('Dimuon.pass == True').select('pseudoweight').show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caching 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP\n",
    "#DF = DF.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DF.where('Dimuon.pass == True').where('Muon_pt[0] > 40').select('pseudoweight','Dimuon').show(20)\n",
    "#DF.where('Dimuon.pass == True').where('Muon_pt[0] > 40').select('pseudoweight','Dijet.mass').show(20)\n",
    "#DF.where('Dimuon.pass == True').where('Muon_pt[0] > 40').select('pseudoweight','DeltaRZjj').show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF.where('Dimuon.pass == True').where('Muon_pt[0] > 40').select('pseudoweight','Dijet.mass','Dijet.pt','Dijet.eta',\n",
    "                                       'Dijet.phi','Dijet.dPhi','Dijet.dR','Dijet.dEta').show(20)\n",
    "#DF.where('Dimuon.pass == True').where('Muon_pt[0] > 40').select('pseudoweight','Dijet.mass').show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get statistics info about the data\n",
    "\n",
    "Exploit pySparkSql functions to get statistical insights on the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "#print 'Number of events, pre-selection level'\n",
    "\n",
    "#DF.groupBy(\"sample\").count().show()\n",
    "\n",
    "#print 'Number of events, Dimuon invariant mass in [70-110] GeV'\n",
    "\n",
    "#DF.where( (col(\"Dimuon.mass\") > 70) & (col(\"Dimuon.mass\") < 110) ).groupBy(\"sample\").count().show()\n",
    "\n",
    "#print 'Mean of Dimuon mass, evaluated in [70-110] GeV range'\n",
    "\n",
    "#DF.where( (col(\"Dimuon.mass\") > 70) & (col(\"Dimuon.mass\") < 110) ).groupBy('sample').mean('Dimuon.mass').show()\n",
    "\n",
    "#print 'Description of Dimuon mass features for SingleMuon dataset only, evaluated in [70-110] GeV range'\n",
    "\n",
    "#DF.where( (col(\"Dimuon.mass\") > 70) & (col(\"Dimuon.mass\") < 110) & (DF[\"sample\"] == \"SingleMuon\") ).describe('Dimuon.mass').show()\n",
    "DF = DF.filter('Muon_pt[0] > 40').filter('Muon_pt[1] > 40')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caching 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DF = DF.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the Zpeak mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries, and append histogrammar functionalities to dataframe\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import histogrammar as hg\n",
    "import histogrammar.sparksql\n",
    "import numpy as np\n",
    "\n",
    "DF = DF.where( (col(\"Dimuon.mass\") > 70) & (col(\"Dimuon.mass\") < 110) )\n",
    "\n",
    "hg.sparksql.addMethods(DF)\n",
    "\n",
    "plots = hg.UntypedLabel(\n",
    "    # 1d histograms\n",
    "    LeadMuPt       = hg.Bin(20, 0, 500,    DF['Dimuon.pt1'], hg.Sum(DF['pseudoweight'])),\n",
    "    LeadMuEta      = hg.Bin(48, -2.4, 2.4, DF['Dimuon.eta1'], hg.Sum(DF['pseudoweight'])),\n",
    "    SubLeadMuPt    = hg.Bin(20, 0, 500,    DF['Dimuon.pt2'], hg.Sum(DF['pseudoweight'])),\n",
    "    SubLeadMuEta   = hg.Bin(48, -2.4, 2.4, DF['Dimuon.eta2'], hg.Sum(DF['pseudoweight'])),\n",
    "    ZInvMass      = hg.Bin(80, 70, 110,   DF['Dimuon.mass'], hg.Sum(DF['pseudoweight'])),\n",
    "    ZDeltaR       = hg.Bin(20, 0, 3,      DF['Dimuon.dPhi'], hg.Sum(DF['pseudoweight'])),\n",
    "    ZDeltaPhi     = hg.Bin(10, 0, 3,      DF['Dimuon.dR'], hg.Sum(DF['pseudoweight'])),\n",
    "    \n",
    "    LeadJetPt       = hg.Bin(20, 0, 500,    DF['Jet_pt'][0], hg.Sum(DF['pseudoweight'])),\n",
    "    LeadJetEta      = hg.Bin(48, -2.4, 2.4, DF['Jet_eta'][0], hg.Sum(DF['pseudoweight'])),\n",
    "    SubLeadJetPt    = hg.Bin(20, 0, 500,    DF['Jet_pt'][1], hg.Sum(DF['pseudoweight'])),\n",
    "    SubLeadJetEta   = hg.Bin(48, -2.4, 2.4, DF['Jet_eta'][1], hg.Sum(DF['pseudoweight'])),\n",
    "    JJInvMass      = hg.Bin(80, 70, 110,   DF['Dijet.mass'], hg.Sum(DF['pseudoweight'])),\n",
    "    JJDeltaR       = hg.Bin(20, 0, 3,      DF['Dijet.dPhi'], hg.Sum(DF['pseudoweight'])),\n",
    "    JJDeltaPhi     = hg.Bin(10, 0, 3,      DF['Dijet.dR'], hg.Sum(DF['pseudoweight'])),\n",
    "    ZJJDeltaR       = hg.Bin(20, 0, 3,      DF['DeltaRZjj'], hg.Sum(DF['pseudoweight'])),\n",
    "    \n",
    ")\n",
    "\n",
    "# Make a set of histograms, categorized per-sample\n",
    "bulkHisto = hg.Categorize(quantity = DF['sample'], value = plots)\n",
    "\n",
    "# Fill from spark\n",
    "bulkHisto.fillsparksql(df=DF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for VARIABLE, num in zip(['ZInvMass','ZDeltaR','ZDeltaPhi','JJInvMass','JJDeltaR','JJDeltaPhi','ZJJDeltaR'],\n",
    "                         ['331','332','333','334','335','336','337']\n",
    "                        ):\n",
    "    \n",
    "    fig = plt.figure(num=None, figsize=(20, 10), dpi=80, facecolor='w', edgecolor='k')\n",
    "\n",
    "    #aHisto   = bulkHisto(\"SingleMuon\")(VARIABLE)\n",
    "    #nBins    = len(aHisto.values)\n",
    "    #edges    = np.linspace(aHisto.low, aHisto.high, nBins + 1)\n",
    "    #width    = (aHisto.high - aHisto.low) / nBins\n",
    "\n",
    "    plotVals = {}\n",
    "    plt.subplot(num)\n",
    "    \n",
    "    for k in bulkHisto.bins:\n",
    "        if k == 'SingleMuon':\n",
    "            continue\n",
    "            \n",
    "        aHisto   = bulkHisto(k)(VARIABLE)\n",
    "        nBins    = len(aHisto.values)\n",
    "        edges    = np.linspace(aHisto.low, aHisto.high, nBins + 1)\n",
    "        width    = (aHisto.high - aHisto.low) / nBins\n",
    "            \n",
    "        #plotVals[k] = [x.toJson()['data']*0.19 for x in bulkHisto(k)(VARIABLE).values]\n",
    "        plotVals[k] = [x.sum for x in bulkHisto(k)(VARIABLE).values]\n",
    "        plt.bar(edges[:-1], plotVals[k], width=width, label=k, color=samples[k]['color'], edgecolor=samples[k]['color'], fill=True)\n",
    "    \n",
    "    xdata   = np.linspace(aHisto.low+0.5*width, aHisto.high+0.5*width, nBins)    \n",
    "    #ydata   = [x.toJson()['data'] for x in bulkHisto('SingleMuon')(VARIABLE).values]\n",
    "    ydata   = [x.sum*4 for x in bulkHisto('SingleMuon')(VARIABLE).values]\n",
    "    yerror  = [x**0.5 for x in ydata]\n",
    "\n",
    "    plt.errorbar(xdata, ydata, fmt='ko', label=\"Data\", xerr=width/2, yerr=yerror, ecolor='black')\n",
    "    \n",
    "    #if VARIABLE[-2:] == \"Pt\":\n",
    "    #    plt.xlabel('Pt of Muon (GeV/c)')\n",
    "    #elif VARIABLE[-4:] == \"Mass\":\n",
    "    #    plt.xlabel('Dimuon invariant mass m($\\mu\\mu$) (GeV)')\n",
    "    #else:\n",
    "    #    plt.xlabel('Angular Unit')\n",
    "    plt.xlabel('%s' %VARIABLE)\n",
    "        \n",
    "    plt.ylabel('Events / %s GeV' %width)\n",
    "    plt.yscale('log')\n",
    "    plt.legend(loc='upper right', fontsize='x-large', prop={'size': 10})\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Learning Extension\n",
    "\n",
    "Preparing dataframe for machin learning. Here we define the set of features distinguishing the signal VH and backgrounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import rand\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "#from pyspark.mllib.linalg import Vectors, VectorUDT\n",
    "vectorizer = udf(lambda x: Vectors.dense(x), VectorUDT())\n",
    "\n",
    "##Construct Desire Features:\n",
    "#nFeatures = vectorizer(\n",
    "#    array(\n",
    "#        DF['Muon_Pt'][0],\n",
    "#        DF['Muon_Eta'][0], \n",
    "#        DF['Jet_Pt'][0], \n",
    "#        DF['Jet_Eta'][0]\n",
    "#    )\n",
    "#) # 4 variables\n",
    "nFeatures = vectorizer(\n",
    "    array(\n",
    "        'Dimuon.mass','Dimuon.dPhi','Dimuon.dR','Dijet.mass','Dijet.dPhi','Dijet.dR','DeltaRZjj'\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "pseudodata = DF.withColumn( \"features\" , nFeatures ).select(\"features\",\"sample\")\n",
    "\n",
    "#Split \n",
    "pseudodata = pseudodata.where( (col('sample') == \"DYJetsToLL\") | (col('sample') == \"ZH\") )\n",
    "#Rename sample to Label\n",
    "dataset = pseudodata.selectExpr(\"features as features\", \"sample as label\").orderBy(rand())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.show(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A quick run through on our cluster landscape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This variable is derived from the number of cores and executors, \n",
    "#and will be used to assign the number of model trainers.\n",
    "\n",
    "num_workers = num_executors * num_cores\n",
    "\n",
    "print(\"Number of desired executors: \" + `num_executors`)\n",
    "print(\"Number of desired cores / executor: \" + `num_cores`)\n",
    "print(\"Total number of workers: \" + `num_workers`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing dataframe\n",
    "\n",
    "StringIndexer --> encodes a string column of labels to a column of label indices. The indices are in [0, numLabels), ordered by label frequencies, so the most frequent label gets index 0\n",
    "\n",
    "StandardScaler --> Apply feature normalization with standard scaling using Spark's StandardScaler. This will transform a feature to have mean 0, and std 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "string_indexer = StringIndexer(inputCol=\"label\", outputCol=\"index_label\")\n",
    "fitted_indexer = string_indexer.fit(dataset)\n",
    "indexed_df = fitted_indexer.transform(dataset)\n",
    "\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withStd=True, withMean=True)\n",
    "fitted_scaler = scaler.fit(indexed_df)\n",
    "scaled_df = fitted_scaler.transform(indexed_df)\n",
    "print(\"The result of indexing and scaling. Each transformation adds new columns to the data frame:\")\n",
    "scaled_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After reading the dataset from storage, we will extract several metrics such as nb_features, which basically is the number of input neurons, and nb_classes, which is the number of classes (signal and background)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_features = len(scaled_df.select(\"features\").take(1)[0][\"features\"])\n",
    "nb_classes = len(scaled_df.select(\"label\").take(1)[0][\"label\"])\n",
    "#nb_classes = 1\n",
    "\n",
    "print(\"Number of features: \" + str(nb_features))\n",
    "print(\"Number of classes: \" + str(nb_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caching training set and test set in memory (WARNING)\n",
    "\n",
    "plit up the dataset for training and testing purposes, and fetch some additional statistics on the number of training and testing instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we create a trainingset and a testset.\n",
    "(training_set, test_set) = scaled_df.randomSplit([0.7, 0.3])\n",
    "#training_set.cache()\n",
    "#test_set.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set.show()\n",
    "test_set.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributing the training and test set to the workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = test_set.repartition(num_workers)\n",
    "training_set = training_set.repartition(num_workers)\n",
    "\n",
    "##release DF\n",
    "DF.unpersist()\n",
    "\n",
    "training_set.cache()\n",
    "test_set.cache()\n",
    "\n",
    "num_test_set = test_set.count()\n",
    "num_training_set = training_set.count()\n",
    "\n",
    "print(\"Number of testset instances: \" + str(num_test_set))\n",
    "print(\"Number of trainingset instances: \" + str(num_training_set))\n",
    "print(\"Total number of instances: \" + str(num_test_set + num_training_set))\n",
    "\n",
    "test_set.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Model Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "model = Sequential()\n",
    "model.add(Dense(500, input_shape=(nb_features,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(500))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.6))\n",
    "model.add(Dense(500))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = 'adagrad'\n",
    "loss = 'categorical_crossentropy'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "from distkeras.transformers import LabelIndexTransformer\n",
    "from distkeras.predictors import ModelPredictor\n",
    "from distkeras.evaluators import *\n",
    "\n",
    "from distkeras.trainers import SingleTrainer\n",
    "#from distkeras.trainers import AEASGD\n",
    "#from distkeras.trainers import DOWNPOUR\n",
    "\n",
    "def evaluate(model):\n",
    "    global test_set\n",
    "\n",
    "    metric_name = \"f1\"\n",
    "    evaluator = MulticlassClassificationEvaluator(metricName=metric_name, predictionCol=\"prediction_index\", labelCol=\"index_label\")\n",
    "    # Clear the prediction column from the testset.\n",
    "    test_set1 = test_set.select(\"scaled_features\", \"label\", \"index_label\")\n",
    "    # Apply a prediction from a trained model.\n",
    "    predictor = ModelPredictor(keras_model=trained_model, features_col=\"scaled_features\")\n",
    "    test_set1 = predictor.predict(test_set1)\n",
    "    # Transform the prediction vector to an indexed label.\n",
    "    index_transformer = LabelIndexTransformer(output_dim=nb_classes)\n",
    "    test_set1 = index_transformer.transform(test_set1)\n",
    "    # Store the F1 score of the SingleTrainer.\n",
    "    score = evaluator.evaluate(test_set1)\n",
    "    \n",
    "    return score\n",
    "\n",
    "def evaluate_accuracy(model, test_set, features=\"scaled_features\"):\n",
    "    evaluator = AccuracyEvaluator(prediction_col=\"prediction_index\", label_col=\"index_label\")\n",
    "    predictor = ModelPredictor(keras_model=trained_model, features_col=features)\n",
    "    transformer = LabelIndexTransformer(output_dim=nb_classes)\n",
    "    test_set2 = test_set.select(features, \"index_label\")\n",
    "    test_set2 = predictor.predict(test_set2)\n",
    "    test_set2 = transformer.transform(test_set2)\n",
    "    score = evaluator.evaluate(test_set2)\n",
    "    \n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "time_spent = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and evaluation: SingleTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SingleTrainer(keras_model=model, loss=loss, worker_optimizer=optimizer, \n",
    "                        features_col=\"scaled_features\", num_epoch=1, batch_size=64)\n",
    "trained_model = trainer.train(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the training time.\n",
    "dt = trainer.get_training_time()\n",
    "print(\"Time spent (SingleTrainer): \" + `dt` + \" seconds.\")\n",
    "\n",
    "# Evaluate the model.\n",
    "score = evaluate(trained_model)\n",
    "print(\"F1 (SingleTrainer): \" + `score`)\n",
    "\n",
    "# Evaluate Accuracy\n",
    "accuracy = evaluate_accuracy(trained_model,test_set)\n",
    "print(\"Accuracy: \" + `accuracy`)\n",
    "\n",
    "# Store the training metrics.\n",
    "results['single'] = score\n",
    "time_spent['single'] = dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
