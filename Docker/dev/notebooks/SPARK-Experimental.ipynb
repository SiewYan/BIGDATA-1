{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up sparkContext in pyspark\n",
    "\n",
    "Web UI\n",
    "- __http://10.64.22.215:8080/__\n",
    "\n",
    "\n",
    "Start a pySpark session including third party packages (SCALA version: 2.11) which will pull from Maven repository:\n",
    "- __spark-root_2.11:0.1.16__ \n",
    "- __histogrammar-sparksql_2.11:1.0.4__\n",
    "- __histbook-1.2.1__\n",
    "\n",
    "Declares external dependency to the Spark application, in case we are using:\n",
    "\n",
    "- __XrootD-Connector__\n",
    "- __XrootD-Connector Grid Certificate__\n",
    "\n",
    "The default cores (defaults at 4) for every spark application is bottlenecked by __SPARK_MASTER_OPTS__ predefined in spark-env.sh. In order to configure spark application beyond the defaults cores, set __spark.cores.max__ in SparkContext before setting:\n",
    "- __spark.executor.instances__\n",
    "- __spark.executor.cores__\n",
    "- __spark.executor.memory__\n",
    "\n",
    "In the example below, a total of __10 cores__ are allocated to Spark with __5 executors__ (__2 cores per executor__) and __2GB RAM__ per executor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version:  2.3.1\n",
      "SparkSQL sesssion created\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql\n",
    "session = pyspark.sql.SparkSession.builder \\\n",
    "    .master('spark://10.64.22.215:7077') \\\n",
    "    .appName('Zpeak_Nanoaod-SPARK') \\\n",
    "    .config('spark.jars.packages','org.diana-hep:spark-root_2.11:0.1.16,org.diana-hep:histogrammar-sparksql_2.11:1.0.4') \\\n",
    "    .config('spark.driver.extraClassPath','/opt/hadoop/share/hadoop/common/lib/EOSfs.jar') \\\n",
    "    .config('spark.executor.extraClassPath','/opt/hadoop/share/hadoop/common/lib/EOSfs.jar') \\\n",
    "    .config('spark.sql.caseSensitive','true') \\\n",
    "    .config('spark.serializer','org.apache.spark.serializer.KryoSerializer') \\\n",
    "    .config(\"spark.cores.max\", \"10\") \\\n",
    "    .config('spark.executor.instances', '5') \\\n",
    "    .config('spark.executor.cores','2') \\\n",
    "    .config('spark.executor.memory','2g') \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "sqlContext = session\n",
    "print 'Spark version: ',sqlContext.version\n",
    "print 'SparkSQL sesssion created'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingesting Nanoaod root file into Dataframe\n",
    "\n",
    "Dataframe is a RDD (Resilient Distributed Dataset) commonly used as an abstraction in Big Data. A Dataframe is an API to the RDD.\n",
    "\n",
    "Root files in NANOAOD format serving from remote CERN public EOS area were read via XrootD-Connector and instantiate in Dataframe. A list of root files (dataset and monte carlo background samples) is defined externally in __samples.py__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ZH sample from EOS file\n",
      "Loading TT sample from EOS file\n",
      "Loading WW sample from EOS file\n",
      "Loading SingleMuon sample from EOS file\n",
      "Loading ZZ sample from EOS file\n",
      "Loading DYJetsToLL sample from EOS file\n",
      "Loading WZ sample from EOS file\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "from samples import *\n",
    "\n",
    "DFList = [] \n",
    "\n",
    "for s in samples:\n",
    "    print 'Loading {0} sample from EOS file'.format(s) \n",
    "    dsPath = \"root://eospublic.cern.ch//eos/opstest/cmspd-bigdata/\"+samples[s]['filename']    \n",
    "    tempDF = sqlContext.read \\\n",
    "                .format(\"org.dianahep.sparkroot\") \\\n",
    "                .option(\"tree\", \"Events\") \\\n",
    "                .load(dsPath)\\\n",
    "                .withColumn(\"pseudoweight\", lit(samples[s]['weight'])) \\\n",
    "                .withColumn(\"sample\", lit(s))                \n",
    "    DFList.append(tempDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Access DataFrame content\n",
    "\n",
    "Return a list of columns in one of the DataFrame, a column corresponds to branche in ROOT TTree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data reduction\n",
    "\n",
    "Subsets of interesting attributes can be selected via 'select' operations on the DataFrames (equivalent to \"pruning\" steps in ROOT-based frameworks).\n",
    "\n",
    "All datasets can be joined into a single DataFrame (e.g. collecting data from various samples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition allocated for Dataframe: 7 partition\n",
      "Partition allocated for Dataframe reported from executors (JVM): 6 partition\n",
      "Default number of partition (defaultParallelism) =  10\n"
     ]
    }
   ],
   "source": [
    "# Define interesting attributes to be selected\n",
    "columns = [\n",
    "    ### MUON\n",
    "    'Muon_pt',\n",
    "    'Muon_eta',\n",
    "    'Muon_phi',\n",
    "    'Muon_mass',\n",
    "    'Muon_charge',\n",
    "    'Muon_mediumId',\n",
    "    'Muon_softId',\n",
    "    'Muon_tightId',\n",
    "    'nMuon',\n",
    "    ### SAMPLE\n",
    "    'sample',\n",
    "    ### Jet\n",
    "    'nJet',\n",
    "    'Jet_pt',\n",
    "    'Jet_eta',\n",
    "    'Jet_phi',\n",
    "    'Jet_mass',\n",
    "    'Jet_bReg',\n",
    "    ### Weight\n",
    "    'pseudoweight',\n",
    "]\n",
    "\n",
    "# Select columns from dataframe\n",
    "DF = DFList[0].select(columns)\n",
    "\n",
    "# Merge all dataset into a single dataframe\n",
    "for df_ in DFList[1:]:\n",
    "    DF = DF.union(df_.select(columns))\n",
    "    \n",
    "print 'Partition allocated for Dataframe:',DF.rdd.getNumPartitions(), 'partition'\n",
    "print 'Partition allocated for Dataframe reported from executors (JVM):',sqlContext._jsc.sc().getExecutorMemoryStatus().size(), 'partition'\n",
    "print 'Default number of partition (defaultParallelism) = ',sqlContext._jsc.sc().defaultParallelism()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-----------+------------+------------+-----------+\n",
      "|    sample|nMuon|    Muon_pt|    Muon_eta|    Muon_phi|Muon_charge|\n",
      "+----------+-----+-----------+------------+------------+-----------+\n",
      "|DYJetsToLL|    1| [34.75507]|[-1.3212891]|[-1.0375977]|        [1]|\n",
      "|DYJetsToLL|    0|         []|          []|          []|         []|\n",
      "|DYJetsToLL|    1|[3.3921983]|[-1.5285645]|[-0.2514038]|       [-1]|\n",
      "|DYJetsToLL|    0|         []|          []|          []|         []|\n",
      "|DYJetsToLL|    0|         []|          []|          []|         []|\n",
      "+----------+-----+-----------+------------+------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DF.filter(DF['sample'] == 'DYJetsToLL')\\\n",
    "  .select('sample','nMuon','Muon_pt','Muon_eta','Muon_phi','Muon_charge')\\\n",
    "  .show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create derivate quantities and structures - 1\n",
    "\n",
    "User defined functions can be used for transformations evalueted row by row to compute derived quantity, such as invaraint mass of two physics objects involving multiple column.\n",
    "The return value is added as a new column in the output DataFrame.\n",
    "\n",
    "Dimuon candidate structure is created as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "dimuonSchema = StructType([\n",
    "    StructField(\"pass\", BooleanType(),False),   # True if filled / False if default(empty) \n",
    "    #\n",
    "    StructField(\"mass\", FloatType(),False),     # Dimuon mass\n",
    "    StructField(\"pt\", FloatType(),False),       # Dimuon pt\n",
    "    StructField(\"eta\", FloatType(),False),      # Dimuon eta\n",
    "    StructField(\"phi\", FloatType(),False),      # Dimuon phi\n",
    "    StructField(\"dPhi\", FloatType(),False),     # DeltaPhi(mu1,mu2)\n",
    "    StructField(\"dR\", FloatType(),False),       # DeltaR(mu1,mu2)\n",
    "    StructField(\"dEta\", FloatType(),False),     # DeltaEta(mu1,mu2)\n",
    "    #\n",
    "    StructField(\"mu1_pt\", FloatType(),False),   # leading mu pT \n",
    "    StructField(\"mu2_pt\", FloatType(),False),   # sub-leading mu pT \n",
    "    StructField(\"mu1_eta\", FloatType(),False),  # leading mu eta\n",
    "    StructField(\"mu2_eta\", FloatType(),False),  # sub-leading mu eta\n",
    "    StructField(\"mu1_phi\", FloatType(),False),  # leading mu phi\n",
    "    StructField(\"mu2_phi\", FloatType(),False),  # sub-leading mu phi\n",
    "])\n",
    "\n",
    "dimuonSchemav2=\"struct<pass: boolean \\\n",
    "                      ,mass: float \\\n",
    "                      ,pt: float \\\n",
    "                      ,eta: float \\\n",
    "                      ,phi: float \\\n",
    "                      ,dPhi: float \\\n",
    "                      ,dR: float \\\n",
    "                      ,dEta: float \\\n",
    "                      ,mu1_pt: float \\\n",
    "                      ,mu2_pt: float \\\n",
    "                      ,mu1_eta: float \\\n",
    "                      ,mu2_eta: float \\\n",
    "                      ,mu1_phi: float \\\n",
    "                      ,mu2_phi: float \\\n",
    "                      >\"\n",
    "\n",
    "dimuonSchemav3=\"array<float>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create derivate quantities and structures - 2\n",
    "\n",
    "And a generic function filling the candidate structure can be defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import function\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "\n",
    "@pandas_udf('double', PandasUDFType.SCALAR)\n",
    "def dimuonCandidate(pt,eta,phi,mass,charge,mediumid,index):\n",
    "#def dimuonCandidate(pt,eta,phi,mass,charge):\n",
    "    #add new column with dimuon information\n",
    "    # default class implementation   \n",
    "    default_ = (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n",
    "    #default_ = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "    \n",
    "    #convert rdd to pdf\n",
    "    #rows = (row_.asDict() for row_ in df_)\n",
    "    #pdf = pd.DataFrame(rows)\n",
    "    \n",
    "    #if len(df) > 0:\n",
    "    if len(pt) > 0:\n",
    "        #pt = df['Muon_pt']\n",
    "        #eta = df['Muon_eta']\n",
    "        #phi = df['Muon_phi']\n",
    "        #mass = df['Muon_mass']\n",
    "        #charge = df['Muon_charge']\n",
    "        #mediumid = df['Muon_mediumId']\n",
    "        \"\"\"\n",
    "        Z->mm candidate from arbitrary muon selection:\n",
    "        N(mu) >= 2\n",
    "        pT > 30, 10\n",
    "        abs(eta) < 2.4, 2.4\n",
    "        mediumId muon\n",
    "        opposite charge\n",
    "        \"\"\"\n",
    "    \n",
    "        if len(pt) < 2:\n",
    "            return default_\n",
    "    \n",
    "        #Identify muon candidate\n",
    "        leadingIdx = None\n",
    "        trailingIdx = None\n",
    " \n",
    "        for idx in range(len(pt)):\n",
    "            if leadingIdx == None:\n",
    "                if pt[idx] > 30 and abs(eta[idx]) < 2.4: #and mediumid[idx]:\n",
    "                    leadingIdx = idx\n",
    "            elif trailingIdx == None:\n",
    "                if pt[idx] > 10 and abs(eta[idx]) < 2.4: #and mediumid[idx]:\n",
    "                    trailingIdx = idx\n",
    "            else:\n",
    "                if pt[idx] > 10 and abs(eta[idx]) < 2.4: #and mediumid[idx]:\n",
    "                    return default_\n",
    "\n",
    "        if leadingIdx != None and trailingIdx != None and charge[leadingIdx] != charge[leadingIdx]:            \n",
    "            # Candidate found\n",
    "            #dimuon_   = (1.0,) + \\\n",
    "            dimuon_    = \\\n",
    "                        [invMass(pt[leadingIdx], pt[trailingIdx],\n",
    "                        eta[leadingIdx], eta[trailingIdx],\n",
    "                        phi[leadingIdx], phi[trailingIdx],\n",
    "                        mass[leadingIdx], mass[trailingIdx]) + \\\n",
    "                        (pt[leadingIdx], pt[trailingIdx],\n",
    "                         eta[leadingIdx], eta[trailingIdx],\n",
    "                         phi[leadingIdx], phi[trailingIdx])]\n",
    "    return dimuon_[index]\n",
    "\n",
    "@pandas_udf('double', PandasUDFType.SCALAR)\n",
    "def pandas_plus_one(m,j):\n",
    "    return m+j\n",
    "\n",
    "def Compute(df_):\n",
    "    #@pandas_udf('double', PandasUDFType.SCALAR)\n",
    "    #def pandas_plus_one(m,j):\n",
    "    #    return m+j\n",
    "    #@pandas_udf('double', PandasUDFType.SCALAR)\n",
    "    #def pandas_plus_two(m,j,k):\n",
    "    #    return m+j+k\n",
    "    \n",
    "    \n",
    "    NEWDF=DF.withColumn(\"mu1_pt\",dimuonCandidate(\n",
    "                                col(\"Muon_pt\"),\n",
    "                                col(\"Muon_eta\"),\n",
    "                                col(\"Muon_phi\"),\n",
    "                                col(\"Muon_mass\"),\n",
    "                                col(\"Muon_charge\"),\n",
    "                                col(\"Muon_mediumId\"),\n",
    "                                lit(1)\n",
    "                              ))\n",
    "    return NEWDF\n",
    "### ArrowNotImplementedError: Not implemented type for list in DataFrameBlock: bool: ARROWWWW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nselection=\"    SELECT Muon_pt,Muon_eta from Events     WHERE nMuon>2 AND Muon_pt[0]>30 AND Muon_pt[1]>10     AND Muon_eta[0] BETWEEN -2.4 AND 2.4     AND Muon_eta[1] BETWEEN -2.4 AND 2.4     AND Muon_mediumId[0] = \\'1\\' AND Muon_mediumId[1] = \\'1\\' \"\\n\\nDF.registerTempTable(\"Events\")\\nsqlContext.sql(\"%s\" %selection).show(5)\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##SQL Querying\n",
    "#selection=\"SELECT nJet,nMuon,Muon_pt from Events \\\n",
    "#WHERE nJet==1 AND nMuon>1 AND Muon_pt[0]>50\"\n",
    "'''\n",
    "selection=\"\\\n",
    "    SELECT Muon_pt,Muon_eta from Events \\\n",
    "    WHERE nMuon>2 AND Muon_pt[0]>30 AND Muon_pt[1]>10 \\\n",
    "    AND Muon_eta[0] BETWEEN -2.4 AND 2.4 \\\n",
    "    AND Muon_eta[1] BETWEEN -2.4 AND 2.4 \\\n",
    "    AND Muon_mediumId[0] = '1' AND Muon_mediumId[1] = '1' \\\n",
    "\"\n",
    "\n",
    "DF.registerTempTable(\"Events\")\n",
    "sqlContext.sql(\"%s\" %selection).show(5)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create derivate quantities and structures - 3\n",
    "\n",
    "Finally, a dimuon candidate structure (an array of defined quantities) can be appended to the DataFrame as an additional column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o400.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 4 times, most recent failure: Lost task 0.3 in stage 3.0 (TID 10, 10.64.22.217, executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark-2.3.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 230, in main\n    process()\n  File \"/opt/spark-2.3.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 225, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/spark-2.3.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 150, in <lambda>\n    func = lambda _, it: map(mapper, it)\n  File \"/opt/spark-2.3.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 280, in load_stream\n    pdf = batch.to_pandas()\n  File \"table.pxi\", line 727, in pyarrow.lib.RecordBatch.to_pandas\n  File \"table.pxi\", line 1120, in pyarrow.lib.Table.to_pandas\n  File \"/usr/lib64/python2.7/site-packages/pyarrow/pandas_compat.py\", line 569, in table_to_blockmanager\n    categories)\n  File \"/usr/lib64/python2.7/site-packages/pyarrow/pandas_compat.py\", line 733, in _table_to_blocks\n    memory_pool, categories)\n  File \"table.pxi\", line 809, in pyarrow.lib.table_to_blocks\n  File \"error.pxi\", line 85, in pyarrow.lib.check_status\nArrowNotImplementedError: Not implemented type for list in DataFrameBlock: bool\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:171)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:121)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat org.apache.spark.sql.execution.python.ArrowEvalPythonExec$$anon$2.<init>(ArrowEvalPythonExec.scala:90)\n\tat org.apache.spark.sql.execution.python.ArrowEvalPythonExec.evaluate(ArrowEvalPythonExec.scala:88)\n\tat org.apache.spark.sql.execution.python.EvalPythonExec$$anonfun$doExecute$1.apply(EvalPythonExec.scala:131)\n\tat org.apache.spark.sql.execution.python.EvalPythonExec$$anonfun$doExecute$1.apply(EvalPythonExec.scala:93)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:800)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:800)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1602)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1590)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1589)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1589)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1823)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1772)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1761)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:363)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3273)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2484)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2484)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3254)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3253)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2484)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2698)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:254)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark-2.3.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 230, in main\n    process()\n  File \"/opt/spark-2.3.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 225, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/spark-2.3.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 150, in <lambda>\n    func = lambda _, it: map(mapper, it)\n  File \"/opt/spark-2.3.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 280, in load_stream\n    pdf = batch.to_pandas()\n  File \"table.pxi\", line 727, in pyarrow.lib.RecordBatch.to_pandas\n  File \"table.pxi\", line 1120, in pyarrow.lib.Table.to_pandas\n  File \"/usr/lib64/python2.7/site-packages/pyarrow/pandas_compat.py\", line 569, in table_to_blockmanager\n    categories)\n  File \"/usr/lib64/python2.7/site-packages/pyarrow/pandas_compat.py\", line 733, in _table_to_blocks\n    memory_pool, categories)\n  File \"table.pxi\", line 809, in pyarrow.lib.table_to_blocks\n  File \"error.pxi\", line 85, in pyarrow.lib.check_status\nArrowNotImplementedError: Not implemented type for list in DataFrameBlock: bool\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:171)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:121)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat org.apache.spark.sql.execution.python.ArrowEvalPythonExec$$anon$2.<init>(ArrowEvalPythonExec.scala:90)\n\tat org.apache.spark.sql.execution.python.ArrowEvalPythonExec.evaluate(ArrowEvalPythonExec.scala:88)\n\tat org.apache.spark.sql.execution.python.EvalPythonExec$$anonfun$doExecute$1.apply(EvalPythonExec.scala:131)\n\tat org.apache.spark.sql.execution.python.EvalPythonExec$$anonfun$doExecute$1.apply(EvalPythonExec.scala:93)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:800)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:800)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-83a5adc56f04>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#test.select(\"addedTHIS\").show()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mu1_pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark-2.3.1-bin-hadoop2.7/python/pyspark/sql/dataframe.pyc\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    348\u001b[0m         \"\"\"\n\u001b[1;32m    349\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.3.1-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.3.1-bin-hadoop2.7/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.3.1-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o400.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 4 times, most recent failure: Lost task 0.3 in stage 3.0 (TID 10, 10.64.22.217, executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark-2.3.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 230, in main\n    process()\n  File \"/opt/spark-2.3.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 225, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/spark-2.3.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 150, in <lambda>\n    func = lambda _, it: map(mapper, it)\n  File \"/opt/spark-2.3.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 280, in load_stream\n    pdf = batch.to_pandas()\n  File \"table.pxi\", line 727, in pyarrow.lib.RecordBatch.to_pandas\n  File \"table.pxi\", line 1120, in pyarrow.lib.Table.to_pandas\n  File \"/usr/lib64/python2.7/site-packages/pyarrow/pandas_compat.py\", line 569, in table_to_blockmanager\n    categories)\n  File \"/usr/lib64/python2.7/site-packages/pyarrow/pandas_compat.py\", line 733, in _table_to_blocks\n    memory_pool, categories)\n  File \"table.pxi\", line 809, in pyarrow.lib.table_to_blocks\n  File \"error.pxi\", line 85, in pyarrow.lib.check_status\nArrowNotImplementedError: Not implemented type for list in DataFrameBlock: bool\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:171)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:121)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat org.apache.spark.sql.execution.python.ArrowEvalPythonExec$$anon$2.<init>(ArrowEvalPythonExec.scala:90)\n\tat org.apache.spark.sql.execution.python.ArrowEvalPythonExec.evaluate(ArrowEvalPythonExec.scala:88)\n\tat org.apache.spark.sql.execution.python.EvalPythonExec$$anonfun$doExecute$1.apply(EvalPythonExec.scala:131)\n\tat org.apache.spark.sql.execution.python.EvalPythonExec$$anonfun$doExecute$1.apply(EvalPythonExec.scala:93)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:800)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:800)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1602)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1590)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1589)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1589)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1823)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1772)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1761)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:363)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3273)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2484)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2484)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3254)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3253)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2484)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2698)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:254)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark-2.3.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 230, in main\n    process()\n  File \"/opt/spark-2.3.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 225, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/spark-2.3.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 150, in <lambda>\n    func = lambda _, it: map(mapper, it)\n  File \"/opt/spark-2.3.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 280, in load_stream\n    pdf = batch.to_pandas()\n  File \"table.pxi\", line 727, in pyarrow.lib.RecordBatch.to_pandas\n  File \"table.pxi\", line 1120, in pyarrow.lib.Table.to_pandas\n  File \"/usr/lib64/python2.7/site-packages/pyarrow/pandas_compat.py\", line 569, in table_to_blockmanager\n    categories)\n  File \"/usr/lib64/python2.7/site-packages/pyarrow/pandas_compat.py\", line 733, in _table_to_blocks\n    memory_pool, categories)\n  File \"table.pxi\", line 809, in pyarrow.lib.table_to_blocks\n  File \"error.pxi\", line 85, in pyarrow.lib.check_status\nArrowNotImplementedError: Not implemented type for list in DataFrameBlock: bool\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:171)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:121)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat org.apache.spark.sql.execution.python.ArrowEvalPythonExec$$anon$2.<init>(ArrowEvalPythonExec.scala:90)\n\tat org.apache.spark.sql.execution.python.ArrowEvalPythonExec.evaluate(ArrowEvalPythonExec.scala:88)\n\tat org.apache.spark.sql.execution.python.EvalPythonExec$$anonfun$doExecute$1.apply(EvalPythonExec.scala:131)\n\tat org.apache.spark.sql.execution.python.EvalPythonExec$$anonfun$doExecute$1.apply(EvalPythonExec.scala:93)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:800)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:800)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "#from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import col\n",
    "#dimuonUDF = udf(dimuonCandidate, dimuonSchema)\n",
    "#scalar \n",
    "#seriesDimuonUDF = pandas_udf(dimuonCandidate, returnType=dimuonSchema)\n",
    "#func_udf = pandas_udf(dimuonCandidate, FloatType(),PandasUDFType.SCALAR)\n",
    "\n",
    "test=Compute(DF)\n",
    "#test.select(\"addedTHIS\").show()\n",
    "test.select(\"mu1_pt\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Return statistical information about the data\n",
    "\n",
    "Exploit pySparkSql functions to get statistical insights on the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "print 'Number of events, pre-selection level'\n",
    "\n",
    "DF.groupBy(\"sample\").count().show()\n",
    "\n",
    "print 'Number of events, Dimuon invariant mass in [70-110] GeV'\n",
    "\n",
    "DF.where( (col(\"Dimuon.mass\") > 70) & (col(\"Dimuon.mass\") < 110) ).groupBy(\"sample\").count().show()\n",
    "\n",
    "print 'Mean of Dimuon mass, evaluated in [70-110] GeV range'\n",
    "\n",
    "DF.where( (col(\"Dimuon.mass\") > 70) & (col(\"Dimuon.mass\") < 110) ).groupBy('sample').mean('Dimuon.mass').show()\n",
    "\n",
    "print 'Description of Dimuon mass features for SingleMuon dataset only, evaluated in [70-110] GeV range'\n",
    "\n",
    "DF.where( (col(\"Dimuon.mass\") > 70) & (col(\"Dimuon.mass\") < 110) & (DF[\"sample\"] == \"SingleMuon\") ).describe('Dimuon.mass').show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the Zpeak mass\n",
    "\n",
    "Finally, the interesting variables are plotted using accumulator from Histogrammar package and the graphic object is handled by backend matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Load libraries, and append histogrammar functionalities to dataframe\n",
    "\n",
    "#%matplotlib inline\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import histogrammar as hg\n",
    "import histogrammar.sparksql\n",
    "import numpy as np\n",
    "\n",
    "DF = DF.where( (col(\"Dimuon.mass\") > 70) & (col(\"Dimuon.mass\") < 110) )\n",
    "\n",
    "hg.sparksql.addMethods(DF)\n",
    "\n",
    "plots = hg.UntypedLabel(\n",
    "    # 1d histograms\n",
    "    LeadPt       = hg.Bin(50, 30, 180,   DF['Dimuon.mu1_pt'],hg.Sum(DF['pseudoweight'])),\n",
    "    LeadPtEta    = hg.Bin(48, -2.4, 2.4, DF['Dimuon.mu1_eta'],hg.Sum(DF['pseudoweight'])),\n",
    "    SubLeadPt    = hg.Bin(100, 0, 200,   DF['Dimuon.mu2_pt'],hg.Sum(DF['pseudoweight'])),\n",
    "    SubLeadPtEta = hg.Bin(48, -2.4, 2.4, DF['Dimuon.mu2_eta'],hg.Sum(DF['pseudoweight'])),\n",
    "    InvMass      = hg.Bin(80, 70, 110,   DF['Dimuon.mass'],hg.Sum(DF['pseudoweight'])),\n",
    "    DeltaR       = hg.Bin(50, 0, 5,      DF['Dimuon.dPhi'],hg.Sum(DF['pseudoweight'])),\n",
    "    DeltaPhi     = hg.Bin(64, -3.2, 3.2, DF['Dimuon.dR'],hg.Sum(DF['pseudoweight'])),\n",
    ")\n",
    "\n",
    "# Make a set of histograms, categorized per-sample\n",
    "bulkHisto = hg.Categorize(quantity = DF['sample'], value = plots)\n",
    "\n",
    "# Fill from spark\n",
    "bulkHisto.fillsparksql(df=DF)\n",
    "print 'Filling histogrammar done'\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# variable for plotting\n",
    "VARIABLE = 'InvMass'\n",
    "\n",
    "fig = plt.figure(num=None, figsize=(8, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "\n",
    "aHisto   = bulkHisto(\"SingleMuon\")(VARIABLE)\n",
    "nBins    = len(aHisto.values)\n",
    "edges    = np.linspace(aHisto.low, aHisto.high, nBins + 1)\n",
    "width    = (aHisto.high - aHisto.low) / nBins\n",
    "\n",
    "plotVals = {}\n",
    "for k in ['DYJetsToLL','ZZ','TT','WW','WZ']:\n",
    "    #plotVals[k] = [x.toJson()['data']*0.19 for x in bulkHisto(k)(VARIABLE).values]\n",
    "    plotVals[k] = [x.sum for x in bulkHisto(k)(VARIABLE).values]\n",
    "    plt.bar(edges[:-1], plotVals[k], width=width, label=k, color=samples[k]['color'], edgecolor=samples[k]['color'], fill=True, log=True)\n",
    "\n",
    "xdata   = np.linspace(aHisto.low+0.5*width, aHisto.high+0.5*width, nBins)    \n",
    "#ydata   = [x.toJson()['data'] for x in bulkHisto('SingleMuon')(VARIABLE).values]\n",
    "ydata   = [x.sum for x in bulkHisto('SingleMuon')(VARIABLE).values]\n",
    "yerror  = [x**0.5 for x in ydata]\n",
    "\n",
    "plt.errorbar(xdata, ydata, fmt='ko', label=\"Data\", xerr=width/2, yerr=yerror, ecolor='black')\n",
    "\n",
    "plt.xlabel('Dimuon invariant mass m($\\mu\\mu$) (GeV)')\n",
    "plt.ylabel('Events / 0.5 GeV')\n",
    "#plt.yscale('log')\n",
    "plt.legend(loc='upper right', fontsize='x-large', )\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from histbook import * \n",
    "#import vega\n",
    "from vega import VegaLite as canvas\n",
    "\n",
    "PROC={}\n",
    "PROCLIST=[\"SingleMuon\",\"WZ\",\"WW\",\"TT\",\"ZZ\",\"DYJetsToLL\"]\n",
    "#cut='cut(\"((col(\"Dimuon.mass\") > 70) & (col(\"Dimuon.mass\") < 110))\")'\n",
    "\n",
    "#Need a flat dataframe\n",
    "dimuon=DF.where( (col(\"Dimuon.mass\") > 70) & (col(\"Dimuon.mass\") < 110) ).select('Dimuon.mu1_pt','Dimuon.mu2_pt','Dimuon.mu1_eta','Dimuon.mu2_eta','Dimuon.mass',\n",
    "                 'Dimuon.dPhi','Dimuon.dR','pseudoweight','sample').persist()\n",
    "\n",
    "#Group filling\n",
    "#Hists = Book(\n",
    "#    Hist(bin(\"mu1_pt\", 50, 30, 180,), weight=\"pseudoweight\"),\n",
    "#    Hist(bin(\"mu1_eta\", 48, -2.4, 2.4), weight=\"pseudoweight\"),\n",
    "#    Hist(bin(\"mu2_pt\", 100, 0, 200), weight=\"pseudoweight\"),\n",
    "#    Hist(bin(\"mu2_eta\", 48, -2.4, 2.4), weight=\"pseudoweight\"),\n",
    "#    Hist(bin(\"mass\", 80, 70, 110), weight=\"pseudoweight\"),\n",
    "#    Hist(bin(\"dPhi\", 50, 0, 5), weight=\"pseudoweight\"),\n",
    "#    Hist(bin(\"dR\", 64, -3.2, 3.2), weight=\"pseudoweight\")\n",
    "#)\n",
    "\n",
    "#fill for each processes\n",
    "for proc in PROCLIST:\n",
    "    PROC[proc]=Book(\n",
    "        Hist(bin(\"mu1_pt\", 50, 30, 180,), weight=\"pseudoweight\"),\n",
    "        Hist(bin(\"mu1_eta\", 48, -2.4, 2.4), weight=\"pseudoweight\"),\n",
    "        Hist(bin(\"mu2_pt\", 100, 0, 200), weight=\"pseudoweight\"),\n",
    "        Hist(bin(\"mu2_eta\", 48, -2.4, 2.4), weight=\"pseudoweight\"),\n",
    "        Hist(bin(\"mass\", 80, 70, 110), weight=\"pseudoweight\"),\n",
    "        Hist(bin(\"dPhi\", 50, 0, 5), weight=\"pseudoweight\"),\n",
    "        Hist(bin(\"dR\", 64, -3.2, 3.2), weight=\"pseudoweight\")\n",
    "    )\n",
    "    PROC[proc].fill(dimuon.where(col('sample') == '%s' %proc ))\n",
    "\n",
    "print \"Fill Done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1=Hist.group(\n",
    "    ZZ=PROC['ZZ'].allvalues()[4],\n",
    "    TT=PROC['TT'].allvalues()[4],\n",
    "    WW=PROC['WW'].allvalues()[4],\n",
    "    WZ=PROC['WZ'].allvalues()[4],\n",
    "    DYJetsToLL=PROC['DYJetsToLL'].allvalues()[4],\n",
    "    #SingleMuon=PROC['SingleMuon'].allvalues()[4]\n",
    ")\n",
    "\n",
    "h1.stack(\"source\",order=PROCLIST).area(\"mass\").to(canvas)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
