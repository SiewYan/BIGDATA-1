{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up sparkContext in pyspark\n",
    "\n",
    "Start a pySpark session including third party packages (SCALA version: 2.11) which will pull from Maven repository:\n",
    "- __spark-root_2.11:0.1.16__ \n",
    "- __histogrammar-sparksql_2.11:1.0.4__\n",
    "\n",
    "Declares external dependency to the Spark application, in case we are using:\n",
    "\n",
    "- __XrootD-Connector__\n",
    "\n",
    "The default cores (defaults at 4) for every spark application is bottlenecked by __SPARK_MASTER_OPTS__ predefined in spark-env.sh. In order to configure spark application beyond the defaults cores, set __spark.cores.max__ in SparkContext before setting:\n",
    "- __spark.executor.instances__\n",
    "- __spark.executor.cores__\n",
    "- __spark.executor.memory__\n",
    "\n",
    "In the example below, a total of __10 cores__ are allocated to Spark with __5 executors__ (__2 cores per executor__) and __2GB RAM__ per executor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version:  2.3.0\n",
      "SparkSQL sesssion created\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql\n",
    "session = pyspark.sql.SparkSession.builder \\\n",
    "    .master('spark://10.64.22.215:7077') \\\n",
    "    .appName('Zpeak_Nanoaod-SPARK') \\\n",
    "    .config('spark.jars.packages','org.diana-hep:spark-root_2.11:0.1.16,org.diana-hep:histogrammar-sparksql_2.11:1.0.4') \\\n",
    "    .config('spark.driver.extraClassPath','/opt/hadoop/share/hadoop/common/lib/EOSfs.jar') \\\n",
    "    .config('spark.executor.extraClassPath','/opt/hadoop/share/hadoop/common/lib/EOSfs.jar') \\\n",
    "    .config('spark.sql.caseSensitive','true') \\\n",
    "    .config('spark.serializer','org.apache.spark.serializer.KryoSerializer') \\\n",
    "    .config(\"spark.cores.max\", \"10\") \\\n",
    "    .config('spark.executor.instances', '5') \\\n",
    "    .config('spark.executor.cores','2') \\\n",
    "    .config('spark.executor.memory','2g') \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "sqlContext = session\n",
    "print 'Spark version: ',sqlContext.version\n",
    "print 'SparkSQL sesssion created'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingesting Nanoaod root file into Dataframe\n",
    "\n",
    "Dataframe is a RDD (Resilient Distributed Dataset) commonly used as an abstraction in Big Data. A Dataframe is an API to the RDD.\n",
    "\n",
    "Root files in NANOAOD format serving from remote CERN public EOS area were read via XrootD-Connector and instantiate in Dataframe. A list of root files (dataset and monte carlo background samples) is defined externally in __samples.py__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ZH sample from EOS file\n",
      "Loading TT sample from EOS file\n",
      "Loading WW sample from EOS file\n",
      "Loading SingleMuon sample from EOS file\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "u'Found duplicate column(s) in the data schema: `Flag_globalTightHalo2016Filter`, `Flag_CSCTightHaloFilter`, `Flag_CSCTightHalo2015Filter`, `Flag_muonBadTrackFilter`, `Flag_globalSuperTightHalo2016Filter`, `Flag_CSCTightHaloTrkMuUnvetoFilter`, `Flag_chargedHadronTrackResolutionFilter`, `Flag_trkPOG_manystripclus53X`, `Flag_eeBadScFilter`, `Flag_trkPOG_logErrorTooManyClusters`, `Flag_HBHENoiseFilter`, `Flag_EcalDeadCellTriggerPrimitiveFilter`, `Flag_HcalStripHaloFilter`, `Flag_hcalLaserEventFilter`, `Flag_METFilters`, `Flag_HBHENoiseIsoFilter`, `Flag_EcalDeadCellBoundaryEnergyFilter`, `Flag_ecalLaserCorrFilter`, `Flag_trkPOG_toomanystripclus53X`, `Flag_trkPOGFilters`, `Flag_goodVertices`;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-36e62e2df901>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m'Loading {0} sample from EOS file'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mdsPath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"root://eospublic.cern.ch//eos/opstest/cmspd-bigdata/\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'filename'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtempDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqlContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m                 \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"org.dianahep.sparkroot\"\u001b[0m\u001b[0;34m)\u001b[0m                 \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tree\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Events\"\u001b[0m\u001b[0;34m)\u001b[0m                 \u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsPath\u001b[0m\u001b[0;34m)\u001b[0m                \u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pseudoweight\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m                 \u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sample\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mDFList\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtempDF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.3.0-bin-hadoop2.7/python/pyspark/sql/readwriter.pyc\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1158\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1160\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.3.0-bin-hadoop2.7/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: u'Found duplicate column(s) in the data schema: `Flag_globalTightHalo2016Filter`, `Flag_CSCTightHaloFilter`, `Flag_CSCTightHalo2015Filter`, `Flag_muonBadTrackFilter`, `Flag_globalSuperTightHalo2016Filter`, `Flag_CSCTightHaloTrkMuUnvetoFilter`, `Flag_chargedHadronTrackResolutionFilter`, `Flag_trkPOG_manystripclus53X`, `Flag_eeBadScFilter`, `Flag_trkPOG_logErrorTooManyClusters`, `Flag_HBHENoiseFilter`, `Flag_EcalDeadCellTriggerPrimitiveFilter`, `Flag_HcalStripHaloFilter`, `Flag_hcalLaserEventFilter`, `Flag_METFilters`, `Flag_HBHENoiseIsoFilter`, `Flag_EcalDeadCellBoundaryEnergyFilter`, `Flag_ecalLaserCorrFilter`, `Flag_trkPOG_toomanystripclus53X`, `Flag_trkPOGFilters`, `Flag_goodVertices`;'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "from samples import *\n",
    "\n",
    "DFList = [] \n",
    "\n",
    "for s in samples:\n",
    "    print 'Loading {0} sample from EOS file'.format(s) \n",
    "    dsPath = \"root://eospublic.cern.ch//eos/opstest/cmspd-bigdata/\"+samples[s]['filename']    \n",
    "    tempDF = sqlContext.read \\\n",
    "                .format(\"org.dianahep.sparkroot\") \\\n",
    "                .option(\"tree\", \"Events\") \\\n",
    "                .load(dsPath)\\\n",
    "                .withColumn(\"pseudoweight\", lit(samples[s]['weight'])) \\\n",
    "                .withColumn(\"sample\", lit(s))                \n",
    "    DFList.append(tempDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Access DataFrame content\n",
    "\n",
    "Return a list of columns in one of the DataFrame, a column corresponds to branche in ROOT TTree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DFList[0].printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data reduction\n",
    "\n",
    "Subsets of interesting attributes can be selected via 'select' operations on the DataFrames (equivalent to \"pruning\" steps in ROOT-based frameworks).\n",
    "\n",
    "All datasets can be joined into a single DataFrame (e.g. collecting data from various samples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define interesting attributes to be selected\n",
    "columns = [\n",
    "    ### MUON\n",
    "    'Muon_pt',\n",
    "    'Muon_eta',\n",
    "    'Muon_phi',\n",
    "    'Muon_mass',\n",
    "    'Muon_charge',\n",
    "    'Muon_mediumId',\n",
    "    'Muon_softId',\n",
    "    'Muon_tightId',\n",
    "    'nMuon',\n",
    "    ### SAMPLE\n",
    "    'sample',\n",
    "    ### Jet\n",
    "    'nJet',\n",
    "    'Jet_pt',\n",
    "    'Jet_eta',\n",
    "    'Jet_phi',\n",
    "    'Jet_mass',\n",
    "    'Jet_bReg',\n",
    "    ### Weight\n",
    "    'pseudoweight',\n",
    "]\n",
    "\n",
    "# Select columns from dataframe\n",
    "DF = DFList[0].select(columns)\n",
    "\n",
    "# Merge all dataset into a single dataframe\n",
    "for df_ in DFList[1:]:\n",
    "    DF = DF.union(df_.select(columns))\n",
    "    \n",
    "print 'Partition allocated for Dataframe:',DF.rdd.getNumPartitions(), 'partition'\n",
    "print 'Partition allocated for Dataframe reported from executors (JVM):',sqlContext._jsc.sc().getExecutorMemoryStatus().size(), 'partition'\n",
    "print 'Default number of partition (defaultParallelism) = ',sqlContext._jsc.sc().defaultParallelism()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Events counting\n",
    "\n",
    "Selection of events based on features is obtained via a 'filter' operation. Number of entries is obtained by 'count'.\n",
    "\n",
    "NOTE: __Counting number of event is computing expensive task!__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "print 'total number of events in the DataFrame  = ', DF.count()\n",
    "print 'events in the DataFrame with \\\"nMuon > 0\\\" = ', DF.filter('nMuon > 0').count()\n",
    "\n",
    "elapsed = timeelapsed = timeit.default_timer() - start_time\n",
    "print 'time elapsed = ',elapsed,' s'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caching - 1\n",
    "\n",
    "Dataframes can be cached into memory, shared across the Spark cluster nodes, for a faster access. By default the dataset is partitioned according to the number of node.\n",
    "\n",
    "Caching is an important aspect of Apache Spark, this permits a swift access to 'hot' dataset which is foreseen to alleviate lot of pain in doing optimization on event selection.\n",
    "\n",
    "The caching will be taken place if an action is invoked on Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print \"Caching will take sometime...\"\n",
    "#DF.cache()\n",
    "DF.persist() # caching in Memory with serialization\n",
    "print 'total number of events in the DataFrame  = ', DF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caching - 2\n",
    "\n",
    "... but ensures __fast data-handling__ operations afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = timeit.default_timer()\n",
    "print 'total number of events in the DataFrame  = ', DF.count()\n",
    "print 'events in the DataFrame with \\\"nMuon > 0\\\" = ', DF.filter('nMuon > 0').count()\n",
    "elapsed = timeelapsed = timeit.default_timer() - start_time\n",
    "print 'time elapsed = ',elapsed,' s'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data inspection\n",
    "\n",
    "Events can be inspected with 'show' (as in TTree.Show() ), also concatenating 'select' and 'filter'.\n",
    "\n",
    "The row correspond to the Event and the column correspond to the variable in the event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF.filter(DF['sample'] == 'DYJetsToLL')\\\n",
    "  .select('sample','nMuon','Muon_pt','Muon_eta','Muon_phi','Muon_charge')\\\n",
    "  .show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create derivate quantities and structures - 1\n",
    "\n",
    "User defined functions can be used for transformations evalueted row by row to compute derived quantity, such as invaraint mass of two physics objects involving multiple column.\n",
    "The return value is added as a new column in the output DataFrame.\n",
    "\n",
    "Dimuon candidate structure is created as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "dimuonSchema = StructType([\n",
    "    StructField(\"pass\", BooleanType(), False),   # True if filled / False if default(empty) \n",
    "    #\n",
    "    StructField(\"mass\", FloatType(), False),     # Dimuon mass\n",
    "    StructField(\"pt\", FloatType(), False),       # Dimuon pt\n",
    "    StructField(\"eta\", FloatType(), False),      # Dimuon eta\n",
    "    StructField(\"phi\", FloatType(), False),      # Dimuon phi\n",
    "    StructField(\"dPhi\", FloatType(), False),     # DeltaPhi(mu1,mu2)\n",
    "    StructField(\"dR\", FloatType(), False),       # DeltaR(mu1,mu2)\n",
    "    StructField(\"dEta\", FloatType(), False),     # DeltaEta(mu1,mu2)\n",
    "    #\n",
    "    StructField(\"mu1_pt\", FloatType(), False),   # leading mu pT \n",
    "    StructField(\"mu2_pt\", FloatType(), False),   # sub-leading mu pT \n",
    "    StructField(\"mu1_eta\", FloatType(), False),  # leading mu eta\n",
    "    StructField(\"mu2_eta\", FloatType(), False),  # sub-leading mu eta\n",
    "    StructField(\"mu1_phi\", FloatType(), False),  # leading mu phi\n",
    "    StructField(\"mu2_phi\", FloatType(), False),  # sub-leading mu phi\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create derivate quantities and structures - 2\n",
    "\n",
    "And a generic function filling the candidate structure can be defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import *\n",
    "\n",
    "def deltaPhi(phi1,phi2):\n",
    "    ## Catch if being called with two objects\n",
    "    if type(phi1) != float and type(phi1) != int:\n",
    "        phi1 = phi1.phi\n",
    "    if type(phi2) != float and type(phi2) != int:\n",
    "        phi2 = phi2.phi\n",
    "    ## Otherwise\n",
    "    dphi = (phi1-phi2)\n",
    "    while dphi >  pi: dphi -= 2*pi\n",
    "    while dphi < -pi: dphi += 2*pi\n",
    "    return dphi\n",
    "\n",
    "def deltaR(eta1,phi1,eta2=None,phi2=None):\n",
    "    ## catch if called with objects\n",
    "    if eta2 == None:\n",
    "        return deltaR(eta1.eta,eta1.phi,phi1.eta,phi1.phi)\n",
    "    ## otherwise\n",
    "    return hypot(eta1-eta2, deltaPhi(phi1,phi2))\n",
    "\n",
    "def invMass(pt1, pt2, eta1, eta2, phi1, phi2, mass1, mass2):\n",
    "    #\n",
    "    theta1 = 2.0*atan(exp(-eta1))\n",
    "    px1    = pt1 * cos(phi1)\n",
    "    py1    = pt1 * sin(phi1)\n",
    "    pz1    = pt1 / tan(theta1)\n",
    "    E1     = sqrt(px1**2 + py1**2 + pz1**2 + mass1**2)\n",
    "\n",
    "    theta2 = 2.0*atan(exp(-eta2))\n",
    "    px2    = pt2 * cos(phi2)\n",
    "    py2    = pt2 * sin(phi2)\n",
    "    pz2    = pt2 / tan(theta2)\n",
    "    E2     = sqrt(px2**2 + py2**2 + pz2**2 + mass2**2)\n",
    "\n",
    "    themass  = sqrt((E1 + E2)**2 - (px1 + px2)**2 - (py1 + py2)**2 - (pz1 + pz2)**2)\n",
    "    thept    = sqrt((px1 + px2)**2 + (py1 + py2)**2)\n",
    "    thetheta = atan( thept / (pz1 + pz2) )        \n",
    "    theeta   = 0.5*log( (sqrt((px1 + px2)**2 + (py1 + py2)**2 + (pz1 + pz2)**2)+(pz1 + pz2))/(sqrt((px1 + px2)**2 + (py1 + py2)**2 + (pz1 + pz2)**2)-(pz1 + pz2)) )\n",
    "    thephi   = asin((py1 + py2)/thept)\n",
    "\n",
    "    delPhi = deltaPhi(phi1,phi2)\n",
    "    delR   = deltaR(eta1,phi1,eta2,phi2)\n",
    "    delEta = eta1-eta2\n",
    "\n",
    "    return (themass, thept, theeta, thephi, delPhi, delR, delEta)\n",
    "\n",
    "def dimuonCandidate(pt, eta, phi, mass, charge, mediumid):\n",
    "    # default class implementation   \n",
    "    default_ = (False, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n",
    "    \n",
    "    \"\"\"\n",
    "    Z->mm candidate from arbitrary muon selection:\n",
    "      N(mu) >= 2\n",
    "      pT > 30, 10\n",
    "      abs(eta) < 2.4, 2.4\n",
    "      mediumId muon\n",
    "      opposite charge\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(pt) < 2:\n",
    "        return default_\n",
    "    \n",
    "    #Identify muon candidate\n",
    "    leadingIdx = None\n",
    "    trailingIdx = None\n",
    " \n",
    "    for idx in range(len(pt)):\n",
    "        if leadingIdx == None:\n",
    "            if pt[idx] > 30 and abs(eta[idx]) < 2.4 and mediumid[idx]:\n",
    "                leadingIdx = idx\n",
    "        elif trailingIdx == None:\n",
    "            if pt[idx] > 10 and abs(eta[idx]) < 2.4 and mediumid[idx]:\n",
    "                trailingIdx = idx\n",
    "        else:\n",
    "            if pt[idx] > 10 and abs(eta[idx]) < 2.4 and mediumid[idx]:\n",
    "                return default_\n",
    "\n",
    "    if leadingIdx != None and trailingIdx != None and charge[leadingIdx] != charge[trailingIdx]:            \n",
    "        # Candidate found\n",
    "        dimuon_   = (True,) + \\\n",
    "                    invMass(pt[leadingIdx], pt[trailingIdx],\n",
    "                            eta[leadingIdx], eta[trailingIdx],\n",
    "                            phi[leadingIdx], phi[trailingIdx],\n",
    "                            mass[leadingIdx], mass[trailingIdx]) + \\\n",
    "                    (pt[leadingIdx], pt[trailingIdx],\n",
    "                     eta[leadingIdx], eta[trailingIdx],\n",
    "                     phi[leadingIdx], phi[trailingIdx])\n",
    "                \n",
    "        return dimuon_\n",
    "    else:\n",
    "        return default_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create derivate quantities and structures - 3\n",
    "\n",
    "Finally, a dimuon candidate structure (an array of defined quantities) can be appended to the DataFrame as an additional column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "dimuonUDF = udf(dimuonCandidate, dimuonSchema)\n",
    "\n",
    "DF = DF.withColumn('Dimuon', dimuonUDF (\"Muon_pt\",\n",
    "                                        \"Muon_eta\",\n",
    "                                        \"Muon_phi\",\n",
    "                                        \"Muon_mass\",\n",
    "                                        \"Muon_charge\",\n",
    "                                        \"Muon_mediumId\")\n",
    "                  )\n",
    "#DF.cache()\n",
    "DF.persist()\n",
    "DF.where('Dimuon.pass == True').select('Dimuon').show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Return statistical information about the data\n",
    "\n",
    "Exploit pySparkSql functions to get statistical insights on the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "print 'Number of events, pre-selection level'\n",
    "\n",
    "DF.groupBy(\"sample\").count().show()\n",
    "\n",
    "print 'Number of events, Dimuon invariant mass in [70-110] GeV'\n",
    "\n",
    "DF.where( (col(\"Dimuon.mass\") > 70) & (col(\"Dimuon.mass\") < 110) ).groupBy(\"sample\").count().show()\n",
    "\n",
    "print 'Mean of Dimuon mass, evaluated in [70-110] GeV range'\n",
    "\n",
    "DF.where( (col(\"Dimuon.mass\") > 70) & (col(\"Dimuon.mass\") < 110) ).groupBy('sample').mean('Dimuon.mass').show()\n",
    "\n",
    "print 'Description of Dimuon mass features for SingleMuon dataset only, evaluated in [70-110] GeV range'\n",
    "\n",
    "DF.where( (col(\"Dimuon.mass\") > 70) & (col(\"Dimuon.mass\") < 110) & (DF[\"sample\"] == \"SingleMuon\") ).describe('Dimuon.mass').show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the Zpeak mass\n",
    "\n",
    "Finally, the interesting variables are plotted using accumulator from Histogrammar package and the graphic object is handled by backend matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries, and append histogrammar functionalities to dataframe\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import histogrammar as hg\n",
    "import histogrammar.sparksql\n",
    "import numpy as np\n",
    "\n",
    "DF = DF.where( (col(\"Dimuon.mass\") > 70) & (col(\"Dimuon.mass\") < 110) )\n",
    "\n",
    "hg.sparksql.addMethods(DF)\n",
    "\n",
    "plots = hg.UntypedLabel(\n",
    "    # 1d histograms\n",
    "    LeadPt       = hg.Bin(50, 30, 180,   DF['Dimuon.mu1_pt'],hg.Sum(DF['pseudoweight'])),\n",
    "    LeadPtEta    = hg.Bin(48, -2.4, 2.4, DF['Dimuon.mu1_eta'],hg.Sum(DF['pseudoweight'])),\n",
    "    SubLeadPt    = hg.Bin(100, 0, 200,   DF['Dimuon.mu2_pt'],hg.Sum(DF['pseudoweight'])),\n",
    "    SubLeadPtEta = hg.Bin(48, -2.4, 2.4, DF['Dimuon.mu2_eta'],hg.Sum(DF['pseudoweight'])),\n",
    "    InvMass      = hg.Bin(80, 70, 110,   DF['Dimuon.mass'],hg.Sum(DF['pseudoweight'])),\n",
    "    DeltaR       = hg.Bin(50, 0, 5,      DF['Dimuon.dPhi'],hg.Sum(DF['pseudoweight'])),\n",
    "    DeltaPhi     = hg.Bin(64, -3.2, 3.2, DF['Dimuon.dR'],hg.Sum(DF['pseudoweight'])),\n",
    ")\n",
    "\n",
    "# Make a set of histograms, categorized per-sample\n",
    "bulkHisto = hg.Categorize(quantity = DF['sample'], value = plots)\n",
    "\n",
    "# Fill from spark\n",
    "bulkHisto.fillsparksql(df=DF)\n",
    "print 'Filling histogrammar done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# variable for plotting\n",
    "VARIABLE = 'InvMass'\n",
    "\n",
    "fig = plt.figure(num=None, figsize=(8, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "\n",
    "aHisto   = bulkHisto(\"SingleMuon\")(VARIABLE)\n",
    "nBins    = len(aHisto.values)\n",
    "edges    = np.linspace(aHisto.low, aHisto.high, nBins + 1)\n",
    "width    = (aHisto.high - aHisto.low) / nBins\n",
    "\n",
    "plotVals = {}\n",
    "for k in ['DYJetsToLL','ZZ','TT','WW','WZ']:\n",
    "    #plotVals[k] = [x.toJson()['data']*0.19 for x in bulkHisto(k)(VARIABLE).values]\n",
    "    plotVals[k] = [x.sum*0.19 for x in bulkHisto(k)(VARIABLE).values]\n",
    "    plt.bar(edges[:-1], plotVals[k], width=width, label=k, color=samples[k]['color'], edgecolor=samples[k]['color'], fill=True, log=True)\n",
    "\n",
    "xdata   = np.linspace(aHisto.low+0.5*width, aHisto.high+0.5*width, nBins)    \n",
    "#ydata   = [x.toJson()['data'] for x in bulkHisto('SingleMuon')(VARIABLE).values]\n",
    "ydata   = [x.sum for x in bulkHisto('SingleMuon')(VARIABLE).values]\n",
    "yerror  = [x**0.5 for x in ydata]\n",
    "\n",
    "plt.errorbar(xdata, ydata, fmt='ko', label=\"Data\", xerr=width/2, yerr=yerror, ecolor='black')\n",
    "\n",
    "plt.xlabel('Dimuon invariant mass m($\\mu\\mu$) (GeV)')\n",
    "plt.ylabel('Events / 0.5 GeV')\n",
    "#plt.yscale('log')\n",
    "plt.legend(loc='upper right', fontsize='x-large', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
