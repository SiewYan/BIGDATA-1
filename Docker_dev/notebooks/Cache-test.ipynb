{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparkContext Configuration\n",
    "\n",
    "The cluster consists of __1 Mesos master node__ and __4 Mesos worker nodes__. The breakdown of the configuration show below:\n",
    "- master node\n",
    "    - 2 Cores\n",
    "    - 4GB RAM\n",
    "- worker node\n",
    "    - 2 cores per node\n",
    "    - 2.7GB RAM per node\n",
    "    \n",
    "There are interesting scenarios to study the performance of spark, which are:\n",
    "- One executor per core:\n",
    "    - instances = (5 nodes x 2 cores); cores = 1 core; memory = (2.9GB/5)\n",
    "- One executor per node:\n",
    "    - instances = 5 nodes; cores = 2 core; memory = 2.9GB/1\n",
    "    \n",
    "NOTE: The default cores (defaults at 4) for every spark application is bottlenecked by __SPARK_MASTER_OPTS__ predefined in spark-env.sh. In order to configure spark application beyond the defaults cores, set __spark.cores.max__ in SparkContext before setting:\n",
    "- __spark.executor.instances__\n",
    "- __spark.executor.cores__\n",
    "- __spark.executor.memory__\n",
    "\n",
    "another NOTE: default parallelism could be set by\n",
    "- .config('spark.default.parallelism','6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n",
      "SparkSQL sesssion created\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql\n",
    "session = pyspark.sql.SparkSession.builder \\\n",
    "    .master('mesos://10.64.22.90:5050') \\\n",
    "    .appName('Cache-test') \\\n",
    "    .config('spark.jars.packages','org.diana-hep:spark-root_2.11:0.1.16,org.diana-hep:histogrammar-sparksql_2.11:1.0.4') \\\n",
    "    .config('spark.driver.extraClassPath','/opt/hadoop/share/hadoop/common/lib/EOSfs.jar') \\\n",
    "    .config('spark.executor.extraClassPath','/opt/hadoop/share/hadoop/common/lib/EOSfs.jar') \\\n",
    "    .config(\"spark.cores.max\", \"8\") \\\n",
    "    .config('spark.executor.instances','4') \\\n",
    "    .config('spark.executor.cores','2') \\\n",
    "    .config('spark.executor.memory','2g') \\\n",
    "    .config('spark.serializer','org.apache.spark.serializer.KryoSerializer') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sqlContext = session\n",
    "print sqlContext.version\n",
    "print 'SparkSQL sesssion created'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Ingestion\n",
    "\n",
    "Nanoaod dataset serving from CERN EOS is ingested in spark via Xrootd Connector (__In the future will be done via grid certificate__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading TT sample from EOS file\n",
      "Loading WW sample from EOS file\n",
      "Loading SingleMuon sample from EOS file\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "u'Found duplicate column(s) in the data schema: `flag_csctighthalofilter`, `flag_trkpog_logerrortoomanyclusters`, `flag_hcallasereventfilter`, `flag_trkpogfilters`, `flag_eebadscfilter`, `flag_muonbadtrackfilter`, `flag_goodvertices`, `flag_ecaldeadcelltriggerprimitivefilter`, `flag_trkpog_manystripclus53x`, `flag_trkpog_toomanystripclus53x`, `flag_hcalstriphalofilter`, `flag_metfilters`, `flag_hbhenoiseisofilter`, `flag_globalsupertighthalo2016filter`, `flag_hbhenoisefilter`, `flag_csctighthalotrkmuunvetofilter`, `flag_csctighthalo2015filter`, `flag_ecallasercorrfilter`, `flag_globaltighthalo2016filter`, `flag_chargedhadrontrackresolutionfilter`, `flag_ecaldeadcellboundaryenergyfilter`;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-f04173379482>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m'Loading {0} sample from EOS file'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mdsPath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"root://eospublic.cern.ch//eos/opstest/cmspd-bigdata/\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'filename'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtempDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqlContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m                 \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"org.dianahep.sparkroot\"\u001b[0m\u001b[0;34m)\u001b[0m                 \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tree\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Events\"\u001b[0m\u001b[0;34m)\u001b[0m                 \u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsPath\u001b[0m\u001b[0;34m)\u001b[0m                \u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sample\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mDFList\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtempDF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.3.0-bin-hadoop2.7/python/pyspark/sql/readwriter.pyc\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1158\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1160\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.3.0-bin-hadoop2.7/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: u'Found duplicate column(s) in the data schema: `flag_csctighthalofilter`, `flag_trkpog_logerrortoomanyclusters`, `flag_hcallasereventfilter`, `flag_trkpogfilters`, `flag_eebadscfilter`, `flag_muonbadtrackfilter`, `flag_goodvertices`, `flag_ecaldeadcelltriggerprimitivefilter`, `flag_trkpog_manystripclus53x`, `flag_trkpog_toomanystripclus53x`, `flag_hcalstriphalofilter`, `flag_metfilters`, `flag_hbhenoiseisofilter`, `flag_globalsupertighthalo2016filter`, `flag_hbhenoisefilter`, `flag_csctighthalotrkmuunvetofilter`, `flag_csctighthalo2015filter`, `flag_ecallasercorrfilter`, `flag_globaltighthalo2016filter`, `flag_chargedhadrontrackresolutionfilter`, `flag_ecaldeadcellboundaryenergyfilter`;'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "from samples import *\n",
    "\n",
    "DFList = [] \n",
    "\n",
    "for s in samples:\n",
    "    print 'Loading {0} sample from EOS file'.format(s) \n",
    "    dsPath = \"root://eospublic.cern.ch//eos/opstest/cmspd-bigdata/\"+samples[s]['filename']    \n",
    "    tempDF = sqlContext.read \\\n",
    "                .format(\"org.dianahep.sparkroot\") \\\n",
    "                .option(\"tree\", \"Events\") \\\n",
    "                .load(dsPath)\\\n",
    "                .withColumn(\"sample\", lit(s))\n",
    "    DFList.append(tempDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "field=([\n",
    "    StructField('FIELDNAME_1',StringType(), True),\n",
    "    StructField('FIELDNAME_2', StringType(), True),\n",
    "    StructField('FIELDNAME_3', StringType(), True)\n",
    "])\n",
    "\n",
    "test=sqlContext.read \\\n",
    "                .format(\"org.dianahep.sparkroot\") \\\n",
    "                .option(\"tree\", \"Events\") \\\n",
    "                .load('root://eospublic.cern.ch//eos/opstest/cmspd-bigdata/SingleElectronRun2016C-03Feb2017-v1.root')\\\n",
    "                .withColumn(\"sample\", lit(s))\n",
    "test.printSchema()\n",
    "schema = StructType(field)\n",
    "sqlContext.sparkContext\n",
    "\n",
    "df = sqlContext.createDataFrame(sqlContext.sparkContext.emptyRDD(), schema)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DFList[0].printSchema()\n",
    "print sqlContext._jsc.sc().getExecutorMemoryStatus().size()\n",
    "print sqlContext._jsc.sc().getExecutorMemoryStatus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming Dataframe\n",
    "\n",
    "- By default, the __available partition (64MB)__ in the cluster (Level of Parallelism) is __equal to the number of cores in all executor__.\n",
    "\n",
    "- By default, the parallelism/partitioning is done __one ingested dataset (Dataframe) per partition__.\n",
    "\n",
    "No Spark Job created after the below cell, concluding that an action invoked on dataframe trigger Spark job creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "    ### MUON\n",
    "    'Muon_pt',\n",
    "    'Muon_eta',\n",
    "    'Muon_phi',\n",
    "    'Muon_mass',\n",
    "    'Muon_charge',\n",
    "    'Muon_mediumId',\n",
    "    'Muon_softId',\n",
    "    'Muon_tightId',\n",
    "    'nMuon',\n",
    "    ### SAMPLE\n",
    "    'sample',\n",
    "]\n",
    "\n",
    "# Select columns from dataframe\n",
    "DF = DFList[0].select(columns)\n",
    "#DF.printSchema()\n",
    "\n",
    "# Merge all dataset into a single dataframe\n",
    "for df_ in DFList[1:]:\n",
    "    DF = DF.union(df_.select(columns))\n",
    "    \n",
    "print 'Partition allocated for Dataframe:',DF.rdd.getNumPartitions(), 'partition'\n",
    "print 'Partition allocated for Dataframe reported from executors (JVM):',sqlContext._jsc.sc().getExecutorMemoryStatus().size(), 'partition'\n",
    "print 'Default number of partition (defaultParallelism) = ',sqlContext._jsc.sc().defaultParallelism()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action1: count\n",
    "\n",
    "compute the number of entry of dataframe without caching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "start_time = timeit.default_timer()\n",
    "print 'total number of row in the DataFrame  = ', DF.count()\n",
    "elapsed = timeelapsed = timeit.default_timer() - start_time\n",
    "print 'time elapsed = ',elapsed,' s'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action2: Cache + count\n",
    "\n",
    "To perform caching of dataframe, the caching will be done on first action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = timeit.default_timer()\n",
    "##DF.cache()\n",
    "DF.persist(pyspark.StorageLevel.MEMORY_ONLY)\n",
    "print 'total number of events in the DataFrame  = ', DF.count()\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print 'time elapsed = ',elapsed,' s'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action3: Count with on Cached dataframe\n",
    "FAST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = timeit.default_timer()\n",
    "print 'total number of row in the DataFrame  = ', DF.count()\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print 'Storage level = ',DF.rdd.getStorageLevel()\n",
    "print 'time elapsed = ',elapsed,' s'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP\n",
    "DF.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = timeit.default_timer()\n",
    "print 'total number of row in the DataFrame  = ', DF.count()\n",
    "elapsed = timeelapsed = timeit.default_timer() - start_time\n",
    "print 'time elapsed = ',elapsed,' s'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action4: count + cache with serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = timeit.default_timer()\n",
    "DF.persist(pyspark.StorageLevel.MEMORY_ONLY_SER)\n",
    "print 'total number of events in the DataFrame  = ', DF.count()\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print 'time elapsed = ',elapsed,' s'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action5: Count with on serially cached dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = timeit.default_timer()\n",
    "print 'total number of row in the DataFrame  = ', DF.count()\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print 'Storage level = ',DF.rdd.getStorageLevel()\n",
    "print 'time elapsed = ',elapsed,' s'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
