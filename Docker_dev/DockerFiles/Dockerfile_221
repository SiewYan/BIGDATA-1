FROM centos:centos7
MAINTAINER Sergio Traldi <sergio.traldi@pd.infn.it>

ARG HADOOP_VERSION=2.6.5
ARG SPARK_VERSION=2.2.1
ARG STR=2.6
#ARG JAVAV=8u161
ARG JAVAV=8u172
ARG JV=172

# Install Essential packages
RUN yum localinstall -y http://repos.mesosphere.com/el/7/noarch/RPMS/mesosphere-el-repo-7-1.noarch.rpm
RUN yum install -y mesos openssh wget python libnss3 curl gzip make which gcc-c++ git
RUN yum --enablerepo=extras install -y epel-release

# Install JDK
RUN wget --no-cookies --no-check-certificate --header "Cookie: gpw_e24=http%3A%2F%2Fwww.oracle.com%2F; oraclelicense=accept-securebackup-cookie" "http://download.oracle.com/otn-pub/java/jdk/${JAVAV}-b11/a58eab1ec242421181065cdc37240b08/jdk-${JAVAV}-linux-x64.tar.gz" -O /opt/jdk-${JAVAV}-linux-x64.tar.gz
RUN tar -zxf /opt/jdk-${JAVAV}-linux-x64.tar.gz -C /opt
RUN ln -sf /opt/jdk1.8.0_${JV} /opt/jdk
RUN rm -rf /opt/jdk-${JAVAV}-linux-x64.tar.gz

RUN echo "net.ipv6.conf.all.disable_ipv6 = 1" >> /etc/sysctl.conf
RUN echo "net.ipv6.conf.default.disable_ipv6 = 1" >> /etc/sysctl.conf
RUN chmod 777 /opt/

# Install Hadoop and Spark
RUN wget http://www.eu.apache.org/dist/hadoop/common/hadoop-${HADOOP_VERSION}/hadoop-${HADOOP_VERSION}.tar.gz -O /opt/hadoop-${HADOOP_VERSION}.tar.gz
RUN tar -zxf /opt/hadoop-${HADOOP_VERSION}.tar.gz -C /opt
RUN ln -sf /opt/hadoop-${HADOOP_VERSION} /opt/hadoop
RUN rm -rf /opt/hadoop-${HADOOP_VERSION}.tar.gz
RUN wget https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop${STR}.tgz -O /opt/spark-${SPARK_VERSION}-bin-hadoop${STR}.tgz
RUN tar -zxf /opt/spark-${SPARK_VERSION}-bin-hadoop${STR}.tgz -C /opt
RUN ln -sf /opt/spark-${SPARK_VERSION}-bin-hadoop${STR} /opt/spark
RUN rm -rf /opt/spark-${SPARK_VERSION}-bin-hadoop${STR}.tgz

# Install XRootD Connector
ENV SPARKHOME /opt/spark
ENV PATH $PATH:/opt/hadoop/bin:/opt/jdk/bin
ENV JAVA_HOME /opt/jdk
RUN wget http://shoh.web.cern.ch/shoh/public/CentOS-CERN.repo -O /etc/yum.repos.d/CentOS-CERN.repo && \
    wget http://shoh.web.cern.ch/shoh/public/RPM-GPG-KEY-cern -O /etc/pki/rpm-gpg/RPM-GPG-KEY-cern && \
    yum install -y xrootd-client xrootd-client-libs xrootd-client-devel

RUN wget http://shoh.web.cern.ch/shoh/public/hadoop-xrootd-connector.tar.gz -O /hadoop-xrootd-connector.tar.gz
RUN tar -zxf /hadoop-xrootd-connector.tar.gz -C /
WORKDIR /hadoop-xrootd-connector
RUN make clean 2>/dev/null && make all
RUN mv /hadoop-xrootd-connector/EOSfs.jar /opt/hadoop/share/hadoop/common/lib/EOSfs.jar
RUN mv /hadoop-xrootd-connector/libjXrdCl.so /opt/hadoop/lib/native/libjXrdCl.so
RUN make clean 2>/dev/null
WORKDIR /
RUN rm -rf hadoop-xrootd-connector
RUN rm /hadoop-xrootd-connector.tar.gz

# Install packages
RUN yum install -y python-pip && \
    pip install --upgrade pip
RUN yum remove -y numpy pyparsing python-enum34
RUN yum install -y graphviz python-devel nano
RUN pip install numpy google-common protobuf
RUN pip install tensorflow pydot keras h5py scikit-learn git+https://github.com/maxpumperla/elephas pip git+https://github.com/JoeriHermans/dist-keras.git flask
#RUN yum install -y python-matplotlib

# Copy scripts
COPY spark-conf/* /opt/spark/conf/
COPY scripts /scripts

# Correct Spark Environment
RUN sed -i 's;spark-XXX-bin-hadoopXX;'spark-${SPARK_VERSION}-bin-hadoop${STR}';g' /opt/spark/conf/spark-env.sh
RUN sed -i 's;py4j-XXXX-src.zip;'py4j-0.10.4-src.zip';g' /opt/spark/conf/spark-env.sh

# clean cache
RUN yum clean all && rm -rf /var/cache/yum/*

ENV SPARKHOME /opt/spark

ENTRYPOINT ["/scripts/run.sh"]